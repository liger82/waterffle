{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "AudioEncoder_poly.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCIE449cfMOY"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "s-ZWoIF4gz5O",
        "outputId": "9ef0db87-2d72-4cb7-fc22-ca8bd76e6d40"
      },
      "source": [
        "!pip uninstall torch torchtext torchvision\n",
        "!pip install torch==1.7.0\n",
        "!pip install torchaudio==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling torch-1.7.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/lib/python3.7/dist-packages/caffe2/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torch-1.7.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled torch-1.7.0\n",
            "Uninstalling torchtext-0.9.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/torchtext-0.9.1.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torchtext/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled torchtext-0.9.1\n",
            "Uninstalling torchvision-0.9.1+cu101:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/torchvision-0.9.1+cu101.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libcudart.c740f4ef.so.10.1\n",
            "    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libjpeg.ceea7512.so.62\n",
            "    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libpng16.7f72a3c5.so.16\n",
            "    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libz.1328edc3.so.1\n",
            "    /usr/local/lib/python3.7/dist-packages/torchvision/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Collecting torch==1.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.7.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.7.0) (1.7.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUww8Ywgq7Qe"
      },
      "source": [
        "# 다 직접 구현한 버전\n",
        "\n",
        "* [https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
        "*[https://github.com/pytorch/fairseq/blob/c36294ea4fd35eac757f417de9668b32c57d4b3d/fairseq/modules/vggblock.py#L22](https://github.com/pytorch/fairseq/blob/c36294ea4fd35eac757f417de9668b32c57d4b3d/fairseq/modules/vggblock.py#L22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR7CfPx92t-1"
      },
      "source": [
        "## ConvolutionEncoder\n",
        "* positional embedding 대신에 conv layer를 거친 입력을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G70Xn0illUAe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Is77ag2nxI"
      },
      "source": [
        "# def infer_conv_output_dim(conv_op, input_dim, sample_inchannel):\n",
        "#     sample_seq_len = 200\n",
        "#     sample_bsz = 10\n",
        "#     x = torch.randn(sample_bsz, sample_inchannel, sample_seq_len, input_dim)\n",
        "#     # N x C x H x W\n",
        "#     # N: sample_bsz, C: sample_inchannel, H: sample_seq_len, W: input_dim\n",
        "#     x = conv_op(x)\n",
        "#     # N x C x H x W\n",
        "#     x = x.transpose(1, 2)\n",
        "#     # N x H x C x W\n",
        "#     bsz, seq = x.size()[:2]\n",
        "#     per_channel_dim = x.size()[3]\n",
        "#     # bsz: N, seq: H, CxW the rest\n",
        "#     return x.contiguous().view(bsz, seq, -1).size(-1), per_channel_dim\n",
        "\n",
        "'''\n",
        "Two 2-D convolutional blocks, each with two conv. layers with kernel size=3, max-pooling kernel=2. The first block has 64 feature maps while the second has 128\n",
        "'''\n",
        "class ConvEncoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 in_channels, \n",
        "                 out_channels,\n",
        "                 conv_kernel_size,\n",
        "                 pooling_kernel_size,\n",
        "                #  input_dim, \n",
        "                 conv_stride=1,\n",
        "                 padding=1,\n",
        "                 layer_norm=True\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # input_channels=1 : mfcc는 2d이므로 채널을 1로 봐야함.\n",
        "        # kernel size 3\n",
        "        conv_op = nn.Conv2d(in_channels=in_channels, out_channels=int(out_channels/2), kernel_size=conv_kernel_size, padding=padding)\n",
        "        self.layers.append(conv_op)\n",
        "        if layer_norm:\n",
        "            self.layers.append(nn.LayerNorm(out_channels))\n",
        "        self.layers.append(nn.ReLU())\n",
        "\n",
        "        conv_op = nn.Conv2d(in_channels=int(out_channels/2), out_channels=out_channels, kernel_size=conv_kernel_size, padding=padding)\n",
        "        self.layers.append(conv_op)\n",
        "        if layer_norm:\n",
        "            self.layers.append(nn.LayerNorm(out_channels))\n",
        "        self.layers.append(nn.ReLU())\n",
        "\n",
        "        # max-pooling kernel=2\n",
        "        # ceil_mode : when True, will use ceil instead of floor to compute the output shape\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=pooling_kernel_size, ceil_mode=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for i, _ in enumerate(self.layers):\n",
        "            x = self.layers[i](x)\n",
        "        return x\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs8AJWy0lbjr",
        "outputId": "2b3888fd-6891-4806-fbc1-2e438b62fafe"
      },
      "source": [
        "ConvEncoder(in_channels=1, \n",
        "            out_channels=64,\n",
        "            conv_kernel_size=3,\n",
        "            pooling_kernel_size=2\n",
        "        #  ,input_dim=30\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvEncoder(\n",
              "  (layers): ModuleList(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIr1g17X29SM"
      },
      "source": [
        "## SelfAttentionEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAr8FS5hrHVl"
      },
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJCb4PZkrV5G"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Determine value outputs\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPXULKa2YXGh",
        "outputId": "30186a13-6229-41a8-bb69-64cfd1a24bcd"
      },
      "source": [
        "MultiheadAttention(input_dim=200\n",
        "                   ,embed_dim=512\n",
        "                   , num_heads=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiheadAttention(\n",
              "  (qkv_proj): Linear(in_features=200, out_features=1536, bias=True)\n",
              "  (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhaL8oHfyfbe"
      },
      "source": [
        "'''\n",
        "(2) 10 encoder transformer blocks all with transformer dim=1024, 16 heads, intermediate ReLU layer size=2048,\n",
        "'''\n",
        "class SelfAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads=8, dropout=0.15):\n",
        "        super().__init__()\n",
        "        # input_dim - Dimensionality of the input\n",
        "        self.mha = MultiheadAttention(input_dim, embed_dim, num_heads)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layerNorm1 = nn.LayerNorm(embed_dim) # 앞선 것과 같은 dim\n",
        "        self.layerNorm2 = nn.LayerNorm(embed_dim) # 앞선 것과 같은 dim\n",
        "        \n",
        "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x1 = self.mha(x)\n",
        "        x1 = self.dropout(x1)\n",
        "\n",
        "        x = torch.cat((x, x1), dim=1)\n",
        "        x = self.layerNorm1(x)\n",
        "\n",
        "        x2 = self.fc(x)\n",
        "        x2 = self.relu(x2)\n",
        "        x2 = self.fc(x)\n",
        "        x2 = self.dropout(x2)\n",
        "\n",
        "        x = torch.cat((x, x2), dim=1)\n",
        "        x = self.layerNorm2(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_SKG2dkcvdZ",
        "outputId": "73d7d498-9e41-446f-d6ad-69bee3524af3"
      },
      "source": [
        "sae = SelfAttentionEncoder(input_dim=200, \n",
        "                           embed_dim=512)\n",
        "sae"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SelfAttentionEncoder(\n",
              "  (mha): MultiheadAttention(\n",
              "    (qkv_proj): Linear(in_features=200, out_features=1536, bias=True)\n",
              "    (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.15, inplace=False)\n",
              "  (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRKBeLrx3GV_"
      },
      "source": [
        "## AudioEncoder 최종"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLBAea5BJ00t"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Any, Dict, Optional, Tuple, Union"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiSRbmd6JZc3"
      },
      "source": [
        "class BasicAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements simple/classical attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 1,\n",
        "        attn: str = 'cosine',\n",
        "        residual: bool = False,\n",
        "        get_weights: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if attn == 'cosine':\n",
        "            self.cosine = nn.CosineSimilarity(dim=dim)\n",
        "        self.attn = attn\n",
        "        self.dim = dim\n",
        "        self.get_weights = get_weights\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        xs: torch.Tensor,\n",
        "        ys: torch.Tensor,\n",
        "        mask_ys: Optional[torch.Tensor] = None,\n",
        "        values: Optional[torch.Tensor] = None,\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Compute attention.\n",
        "        Attend over ys with query xs to obtain weights, then apply weights to\n",
        "        values (ys if yalues is None)\n",
        "        Args:\n",
        "            xs: B x query_len x dim (queries)\n",
        "            ys: B x key_len x dim (keys)\n",
        "            mask_ys: B x key_len (mask)\n",
        "            values: B x value_len x dim (values); if None, default to ys\n",
        "        \"\"\"\n",
        "        bsz = xs.size(0)\n",
        "        y_len = ys.size(1)\n",
        "        x_len = xs.size(1)\n",
        "        if self.attn == 'cosine':\n",
        "            l1 = self.cosine(xs, ys).unsqueeze(self.dim - 1)\n",
        "        else:\n",
        "            l1 = torch.bmm(xs, ys.transpose(1, 2))\n",
        "            if self.attn == 'sqrt':\n",
        "                d_k = ys.size(-1)\n",
        "                l1 = l1 / math.sqrt(d_k)\n",
        "        if mask_ys is not None:\n",
        "            attn_mask = (mask_ys == 0).view(bsz, 1, y_len)\n",
        "            attn_mask = attn_mask.repeat(1, x_len, 1)\n",
        "            l1.masked_fill_(attn_mask, neginf(l1.dtype))\n",
        "        l2 = F.softmax(l1, dim=self.dim, dtype=torch.float).type_as(l1)\n",
        "        if values is None:\n",
        "            values = ys\n",
        "        lhs_emb = torch.bmm(l2, values)\n",
        "\n",
        "        # # add back the query\n",
        "        if self.residual:\n",
        "            lhs_emb = lhs_emb.add(xs)\n",
        "\n",
        "        if self.get_weights:\n",
        "            return lhs_emb.squeeze(self.dim - 1), l2\n",
        "        else:\n",
        "            return lhs_emb.squeeze(self.dim - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxHozUOSJvrb"
      },
      "source": [
        "class PolyBasicAttention(BasicAttention):\n",
        "    \"\"\"\n",
        "    Override basic attention to account for edge case for polyencoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, poly_type, n_codes, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.poly_type = poly_type\n",
        "        self.n_codes = n_codes\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        Account for accidental dimensionality reduction when num_codes is 1 and the\n",
        "        polyencoder type is 'codes'\n",
        "        \"\"\"\n",
        "        lhs_emb = super().forward(*args, **kwargs)\n",
        "        if self.poly_type == 'codes' and self.n_codes == 1 and len(lhs_emb.shape) == 2:\n",
        "            lhs_emb = lhs_emb.unsqueeze(self.dim - 1)\n",
        "        return lhs_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFqLnW9_KClS"
      },
      "source": [
        "\"\"\"Near infinity, useful as a large penalty for scoring when inf is bad.\"\"\"\n",
        "NEAR_INF = 1e20\n",
        "NEAR_INF_FP16 = 65504\n",
        "\n",
        "def neginf(dtype: torch.dtype) -> float:\n",
        "    \"\"\"\n",
        "    Return a representable finite number near -inf for a dtype.\n",
        "    \"\"\"\n",
        "    if dtype is torch.float16:\n",
        "        return -NEAR_INF_FP16\n",
        "    else:\n",
        "        return -NEAR_INF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O1V3Nfa6Yzm"
      },
      "source": [
        "class AudioEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 poly_n_codes, # m, the number of global contxt features\n",
        "                #  poly_attention_type,\n",
        "                 poly_attention_num_heads,\n",
        "                #  codes_attention_type,\n",
        "                 codes_attention_num_heads,\n",
        "                 embed_dim, \n",
        "\n",
        "                 num_conv_layers, \n",
        "                 in_channels, \n",
        "                 conv_kernel_size=3, \n",
        "                 pooling_kernel_size=2,\n",
        "                 num_att_layers=6, \n",
        "                 dropout=0.15):\n",
        "        super(AudioEncoder, self).__init__()\n",
        "\n",
        "        self.n_codes = poly_n_codes\n",
        "        self.attention_num_heads = poly_attention_num_heads\n",
        "        self.codes_attention_num_heads = codes_attention_num_heads\n",
        "\n",
        "        # the codes\n",
        "        codes = torch.empty(self.n_codes, embed_dim)\n",
        "        codes = torch.nn.init.uniform_(codes)\n",
        "        self.codes = torch.nn.Parameter(codes)\n",
        "\n",
        "        # attention for the codes\n",
        "        self.code_attention = PolyBasicAttention(poly_type='codes', n_codes=self.n_codes, dim=2, attn='basic', get_weights=False)\n",
        "\n",
        "        # The final attention (the one that takes the candidate as key)\n",
        "        self.attention = MultiheadAttention(embed_dim, embed_dim, self.attention_num_heads)\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "\n",
        "        # two 2d-conv blocks\n",
        "        # in_channels=1, \n",
        "        # out_channels=64,\n",
        "        # conv_kernel_size=3,\n",
        "        # pooling_kernel_size=2,\n",
        "        for i in range(num_conv_layers):\n",
        "            self.encoder.append(\n",
        "                ConvEncoder(\n",
        "                    in_channels=in_channels, \n",
        "                    out_channels=embed_dim,\n",
        "                    conv_kernel_size=conv_kernel_size,\n",
        "                    pooling_kernel_size=pooling_kernel_size\n",
        "                    )\n",
        "                )\n",
        "        \n",
        "        # SelfAttentionEncoder * num_att_layers\n",
        "        for i in range(num_att_layers):\n",
        "            self.encoder.append(\n",
        "                SelfAttentionEncoder(\n",
        "                    input_dim=embed_dim,\n",
        "                    embed_dim=embed_dim, \n",
        "                    num_heads=self.attention_num_heads, \n",
        "                    dropout=dropout)\n",
        "                )\n",
        "            \n",
        "    def attend(self, attention_layer, queries, keys, values, mask):\n",
        "        \"\"\"\n",
        "        Apply attention.\n",
        "        :param attention_layer:\n",
        "            nn.Module attention layer to use for the attention\n",
        "        :param queries:\n",
        "            the queries for attention\n",
        "        :param keys:\n",
        "            the keys for attention\n",
        "        :param values:\n",
        "            the values for attention\n",
        "        :param mask:\n",
        "            mask for the attention keys\n",
        "        :return:\n",
        "            the result of applying attention to the values, with weights computed\n",
        "            wrt to the queries and keys.\n",
        "        \"\"\"\n",
        "        if keys is None:\n",
        "            keys = values\n",
        "        if isinstance(attention_layer, PolyBasicAttention):\n",
        "            return attention_layer(queries, keys, mask_ys=mask, values=values)\n",
        "        elif isinstance(attention_layer, MultiHeadAttention):\n",
        "            return attention_layer(queries, keys, values, mask)[0]\n",
        "        else:\n",
        "            raise Exception('Unrecognized type of attention')\n",
        "\n",
        "                \n",
        "    def forward(self, x, label=None):\n",
        "        # x = [ctxt, next]\n",
        "        # next: candidate\n",
        "        ctxt, next = x\n",
        "        \n",
        "        # cand mfcc를 conv->selfattention encoder를 거친 emb\n",
        "        cand_emb = self.encoder(next)\n",
        "\n",
        "        # ctxt mfcc를 conv->self attention encoder를 거친 emb\n",
        "        ctxt_out = self.encoder(next)\n",
        "\n",
        "        # m개 만큼 context code를 반복\n",
        "        # ctxt_out 값과 code를 내적한 값들의 softmax한 벡터 (w_1,...,w_m)를 이전 레이어 결과값(ctxt_out)과 곱해서 합한다.\n",
        "        # 이 값이 m개의 global context features\n",
        "        bsz = cand_emb.size(0)\n",
        "        global_ctxts = self.attend(attention_layer=self.code_attention , \n",
        "                                   queries=self.codes.repeat(bsz, 1, 1), \n",
        "                                   keys=ctxt_out,\n",
        "                                   values=ctxt_out, \n",
        "                                   mask=None)        \n",
        "\n",
        "        # m개의 global context features를 cand_emb와 내적한 값을 softmax한 벡터를 (w_1,...,w_m)라 할 때, 이 가중치 값과 global contxt features를 곱해서 합한다.\n",
        "        # 이 값이 최종 ctxt_emb\n",
        "        ctxt_emb = self.attend(attention_layer=self.attention ,\n",
        "                               queries=cand_emb,\n",
        "                               keys=global_ctxts,\n",
        "                               values=global_ctxts,\n",
        "                               mask=None)        \n",
        "\n",
        "        # score: cand_emb와 ctxt_emb 간 cosine similarity값 (반환값)\n",
        "        if label is None:\n",
        "            scores = torch.sum(ctxt_emb * cand_emb, 2)\n",
        "            return scores\n",
        "        else:\n",
        "            # we are recycling responses for faster training\n",
        "            # we repeat responses for batch_size times to simulate test phase\n",
        "            # so that every context is paired with batch_size responses\n",
        "            cand_emb = cand_emb.permute(1, 0, 2) # [1, bs, dim]\n",
        "            cand_emb = cand_emb.expand(bsz, bsz, cand_emb.shape[2]) # [bs, bs, dim]\n",
        "            ctxt_emb = self.attend(attention_layer=self.attention, \n",
        "                                   queries=cand_emb, \n",
        "                                   keys=global_ctxts,\n",
        "                                   values=global_ctxts,\n",
        "                                   mask=None)        \n",
        "            ctxt_emb = (cand_emb, embs, embs).squeeze() # [bs, bs, dim]\n",
        "            dot_product = torch.sum(ctxt_emb * cand_emb, 2)\n",
        "            loss = F.log_softmax(dot_product, dim=-1)\n",
        "            loss = (-loss.sum(dim=1)).mean()\n",
        "            return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8SrkyurGPu2",
        "outputId": "811c69aa-1ee5-43c7-fb6a-555c350fa3ec"
      },
      "source": [
        "AudioEncoder(poly_n_codes=16, # m, the number of global contxt features / 16, 64, 360\n",
        "            #  poly_attention_type, # MultiHeadAttention으로 고정\n",
        "            poly_attention_num_heads=8,\n",
        "            #  codes_attention_type, # PolyBasicAttention으로 고정\n",
        "            codes_attention_num_heads=2,\n",
        "            embed_dim=512, \n",
        "            num_conv_layers=2, \n",
        "            in_channels=1, \n",
        "            conv_kernel_size=3, \n",
        "            pooling_kernel_size=2,\n",
        "            num_att_layers=6, \n",
        "            dropout=0.15)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AudioEncoder(\n",
              "  (code_attention): PolyBasicAttention()\n",
              "  (attention): MultiheadAttention(\n",
              "    (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
              "    (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (encoder): ModuleList(\n",
              "    (0): ConvEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    )\n",
              "    (1): ConvEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    )\n",
              "    (2): SelfAttentionEncoder(\n",
              "      (mha): MultiheadAttention(\n",
              "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): SelfAttentionEncoder(\n",
              "      (mha): MultiheadAttention(\n",
              "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): SelfAttentionEncoder(\n",
              "      (mha): MultiheadAttention(\n",
              "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): SelfAttentionEncoder(\n",
              "      (mha): MultiheadAttention(\n",
              "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): SelfAttentionEncoder(\n",
              "      (mha): MultiheadAttention(\n",
              "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): SelfAttentionEncoder(\n",
              "      (mha): MultiheadAttention(\n",
              "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LTjEHiv1YRC"
      },
      "source": [
        "# hyperparameters\n",
        "* [원논문](https://arxiv.org/pdf/1904.11660.pdf)\n",
        "* For model optimization, we use the AdaDelta algorithm with fixed learning rate=1.0 and gradient clipping at 10.0. \n",
        "* 80 epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2eu1tvS_2Zc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}