{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f5d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/home/super/waterffle/playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223052c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d31fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 메모리 일부로 제한\n",
    "# torch.cuda.set_per_process_memory_fraction(0.5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2384a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efbf1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_ausil_feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "639b0d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Get extracted feature by pre-trained CNN ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32000/32000 [00:22<00:00, 1443.49it/s]\n"
     ]
    }
   ],
   "source": [
    "cs, ns, labels = load_ausil_feature_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca15b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsz * 10 개만 할당\n",
    "part = 3200\n",
    "cs = torch.tensor(cs[:part])\n",
    "ns = torch.tensor(ns[:part])\n",
    "labels = torch.tensor(labels[:part])\n",
    "\n",
    "# cs = torch.tensor(cs)\n",
    "# ns = torch.tensor(ns)\n",
    "# labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7f5beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3200, 1, 2528]), torch.Size([3200, 1, 2528]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.size(), ns.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "108869a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3200, 2528, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = ns.transpose(1,2)\n",
    "ns.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebdc9967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3200, 1, 2528]), torch.Size([3200, 2528, 1]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.size(), ns.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "620b5bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2528])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4fccc",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001656f",
   "metadata": {},
   "source": [
    "# load the features from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0cc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93e76da9",
   "metadata": {},
   "source": [
    "# 1. just compute cosine similarity between cs and ns of the features (no train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCS(nn.Module):\n",
    "    '''\n",
    "    PLM based Cosine Similarity\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(PCS, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ctxt_emb, cand_emb = x\n",
    "        \n",
    "        # score: cand_emb와 ctxt_emb 간 cosine similarity값 (반환값)\n",
    "        scores = torch.sum(ctxt_emb * cand_emb, -1)\n",
    "        print(f\"scores : {scores.size()}\")\n",
    "        scores = scores.view(1, -1)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c243d",
   "metadata": {},
   "source": [
    "# 2. PLM features -> transformer -> cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCT(nn.Module):\n",
    "    '''\n",
    "    PLM based Cosine Similarity \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 embed_dim,\n",
    "                 attention_num_heads,\n",
    "                 dropout,\n",
    "                 activation='gelu'\n",
    "                ):\n",
    "        super(PCT, self).__init__()\n",
    "        \n",
    "        self.att_encoder_block = nn.ModuleList()\n",
    "        \n",
    "        # conv encoder를 거쳐나온 데이터의 dim과 embed_dim이 다를 경우 맞춰준다\n",
    "        if input_dim != embed_dim:\n",
    "            self.att_encoder_block.append(nn.Linear(input_dim, embed_dim))\n",
    "        \n",
    "        # SelfAttentionEncoder * num_att_layers\n",
    "        for i in range(num_att_layers):\n",
    "            self.att_encoder_block.append(\n",
    "                torch.nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                 nhead=self.attention_num_heads, \n",
    "                                                 dim_feedforward=embed_dim*4, \n",
    "                                                 dropout=dropout, \n",
    "                                                 activation=activation)\n",
    "            )\n",
    "                            \n",
    "        self.att_encoder_block = nn.Sequential(*self.att_encoder_block)\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ctxt_emb, cand_emb = x\n",
    "        \n",
    "        # score: cand_emb와 ctxt_emb 간 cosine similarity값 (반환값)\n",
    "        scores = torch.sum(ctxt_emb * cand_emb, -1)\n",
    "#         print(f\"scores : {scores.size()}\")\n",
    "        scores = scores.view(1, -1)\n",
    "        \n",
    "        pred = self.att_encoder_block(scores)\n",
    "    \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be8599",
   "metadata": {},
   "source": [
    "# 3. PLM features -> Poly Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5639e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6b2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ff65bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "from operator import itemgetter\n",
    "\n",
    "class CnnFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnFeatureExtractor, self).__init__()\n",
    "        self.globalpool = F.max_pool2d # can use max also\n",
    "\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(1,16,kernel_size=3,padding=1),nn.BatchNorm2d(16),nn.GELU())\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(16,16,kernel_size=3,padding=1),nn.BatchNorm2d(16),nn.GELU())\n",
    "        self.layer3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(16,32,kernel_size=3,padding=1),nn.BatchNorm2d(32),nn.GELU())\n",
    "        self.layer5 = nn.Sequential(nn.Conv2d(32,32,kernel_size=3,padding=1),nn.BatchNorm2d(32),nn.GELU())\n",
    "        self.layer6 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.layer7 = nn.Sequential(nn.Conv2d(32,64,kernel_size=3,padding=1),nn.BatchNorm2d(64),nn.GELU())\n",
    "        self.layer8 = nn.Sequential(nn.Conv2d(64,64,kernel_size=3,padding=1),nn.BatchNorm2d(64),nn.GELU())\n",
    "        self.layer9 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.layer10 = nn.Sequential(nn.Conv2d(64,128,kernel_size=3,padding=1),nn.BatchNorm2d(128),nn.GELU())\n",
    "        self.layer11 = nn.Sequential(nn.Conv2d(128,128,kernel_size=3,padding=1),nn.BatchNorm2d(128),nn.GELU())\n",
    "        self.layer12 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.layer13 = nn.Sequential(nn.Conv2d(128,256,kernel_size=3,padding=1),nn.BatchNorm2d(256),nn.GELU())\n",
    "        self.layer14 = nn.Sequential(nn.Conv2d(256,256,kernel_size=3,padding=1),nn.BatchNorm2d(256),nn.GELU())\n",
    "        self.layer15 = nn.MaxPool2d(2) #\n",
    "\n",
    "        self.layer16 = nn.Sequential(nn.Conv2d(256,512,kernel_size=3,padding=1),nn.BatchNorm2d(512),nn.GELU())\n",
    "        self.layer17 = nn.MaxPool2d(2) # \n",
    "        \n",
    "        self.layer18 = nn.Sequential(nn.Conv2d(512,1024,kernel_size=2),nn.BatchNorm2d(1024),nn.GELU())\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = self.layer14(out)\n",
    "        out = self.layer15(out)\n",
    "        out = self.layer16(out)\n",
    "        out = self.layer17(out)\n",
    "        out1 = self.layer18(out)\n",
    "        \n",
    "        out = self.globalpool(out1,kernel_size=out1.size()[2:])\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return out #,out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22d18fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WF, self).__init__()\n",
    "        \n",
    "        # context encoder \n",
    "        self.ctxt_encoder = CnnFeatureExtractor()\n",
    "        \n",
    "        # candidate encoder\n",
    "        self.cand_encoder = CnnFeatureExtractor()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cs, ns = x\n",
    "        \n",
    "        ctxt_emb = self.ctxt_encoder(cs)\n",
    "        cand_emb = self.ctxt_encoder(ns)\n",
    "        \n",
    "        # score: cand_emb와 ctxt_emb 간 cosine similarity값 (반환값)\n",
    "        scores = torch.sum(ctxt_emb * cand_emb, -1)\n",
    "#         print(f\"scores : {scores.size()}\")\n",
    "        scores = scores.view(1, -1)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f1a59a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd06af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072292ab",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fd1fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion, device):\n",
    "    # layer 중 dropout layer와 같이 학습 시에는 사용하는데 inference할 때는 사용하지 않는 경우를 구분해주기 위함\n",
    "    model.eval()\n",
    "    \n",
    "    n_steps = 0\n",
    "    eval_loss = 0    \n",
    "    acc = 0\n",
    "\n",
    "    # autograd engine을 끔(gradient 계산하지 않음) --> 메모리 사용량을 줄이고 연산 속도를 높이기 위함\n",
    "    with torch.no_grad():\n",
    "        # input은 이미 1차 임베딩은 되어 있는 것을 가져왔다고 가정한다.\n",
    "        for idx, (batch_cs, batch_ns, batch_label) in enumerate(dataloader):\n",
    "            n_steps+=1\n",
    "            batch_label = torch.LongTensor([torch.argmax(batch_label).item()]).to(device)\n",
    "            scores = model(x=[batch_cs, batch_ns]).detach()\n",
    "            \n",
    "#             print(f'eval score : {scores}')\n",
    "                \n",
    "            loss = criterion(scores, batch_label)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "#             print(f'predict : {torch.argmax(scores, axis=1)}')\n",
    "#             print(f'label  : {batch_label}')\n",
    "            if torch.argmax(scores, axis=1) == batch_label:\n",
    "                acc += 1\n",
    "    if n_steps == 0:\n",
    "        raise Exception('Eval Metric must have at least one example before it can be computed.')\n",
    "    results = {\n",
    "        'eval_accuracy': acc / n_steps,\n",
    "        'eval_loss': eval_loss / n_steps\n",
    "    }\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11faa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def train(model, raw_data, epochs, batch_size, output_dir, lr, device='cpu', betas=(0.9, 0.999), weight_decay: float = 0.01, val_ratio=0.2, fp16=False, fp16_opt_level='O1'):\n",
    "    import datetime\n",
    "    import time\n",
    "    # fp16=True면 amp를 통한 mixed preicision training을 한다는 의미\n",
    "    # 사용 조건 : Volta 이상의 nvidia 그래픽 카드(v100, rtx2080ti, 등)\n",
    "    if fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=fp16_opt_level)\n",
    "    \n",
    "    def train_batch(data, bsz, shuffle=True):\n",
    "        cs_rep, ns_rep, labels = data\n",
    "        length = cs_rep.size()[0]\n",
    "        \n",
    "        for i in range(0, length, bsz):\n",
    "            cs_batch = cs_rep[i:i+bsz]\n",
    "            ns_batch = ns_rep[i:i+bsz]\n",
    "            label_batch = labels[i:i+bsz]\n",
    "            \n",
    "            if shuffle:\n",
    "                indexes = torch.randperm(bsz)              \n",
    "                yield [cs_batch[indexes], ns_batch[indexes], label_batch[indexes]]\n",
    "            else:\n",
    "                yield [cs_batch, ns_batch, label_batch]    \n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    state_save_path = os.path.join(output_dir, '{}_{}_{}_pytorch_model.bin'.format('bi-encoder', 'mls', now))\n",
    "        \n",
    "    best_eval_loss = float('inf')\n",
    "    best_test_loss = float('inf')\n",
    "        \n",
    "    cs, ns, labels = raw_data[0].to(device), raw_data[1].to(device), raw_data[2].to(device)\n",
    "\n",
    "    # 재사용 가능한 임베딩들은 미리 뽑기\n",
    "    trainset = None\n",
    "    valset = None\n",
    "\n",
    "    if val_ratio > 0:\n",
    "        train_len = int((1-val_ratio)*len(labels))\n",
    "        trainset = [cs[:train_len], ns[:train_len], labels[:train_len]]\n",
    "        valset = [cs[train_len:], ns[train_len:], labels[train_len:]]\n",
    "    else:\n",
    "        trainset = raw_data\n",
    "\n",
    "    eval_freq = (len(trainset[2]) / batch_size) // 3\n",
    "    \n",
    "    t_total = len(trainset[0])\n",
    "    \n",
    "#     optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, num_warmup_steps=100, num_training_steps=t_total\n",
    "#     )\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "\n",
    "    # Set the learning rate of each parameter group using a cosine annealing schedule,\n",
    "    # https://pytorch.org/docs/1.8.0/optim.html?highlight=cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    # stepRL\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=eval_freq, gamma=0.95)\n",
    "\n",
    "        \n",
    "    batch_labels = []\n",
    "    scores = []\n",
    "    global_step = 0\n",
    "    #TODO : 나중에 tqdm으로 바꾸기\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        train_batches = train_batch(trainset, batch_size)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        local_step = 0\n",
    "        for idx, (batch_cs, batch_ns, batch_label) in enumerate(train_batches):\n",
    "            model.train()\n",
    "            \n",
    "            # label\n",
    "            batch_label = torch.LongTensor([torch.argmax(batch_label).item()]).to(device)\n",
    "            \n",
    "            # dot product\n",
    "            scores = model([batch_cs, batch_ns])\n",
    "#             print(f'scores : {scores}')\n",
    "\n",
    "            # cross entropy\n",
    "            loss = criterion(scores, batch_label)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predict = torch.argmax(scores, axis=1)\n",
    "#             print('predict : ',predict.item())\n",
    "#             print(f'label : {batch_label.item()}\\n')\n",
    "#             if predict == torch.argmax(batch_label).item():\n",
    "            if predict == batch_label:\n",
    "                accuracy += 1\n",
    "#                 print(f'-- correct : {accuracy}\\n')\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(f'scores : {scores}')\n",
    "                exit()\n",
    "            \n",
    "            if fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "#             scheduler.step()\n",
    "\n",
    "            # clear gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "#             model.zero_grad()\n",
    "            \n",
    "#             if global_step and global_step % eval_freq == 0:\n",
    "#                 if valset is not None:\n",
    "#                     val_batches = train_batch(valset, batch_size, shuffle=False)\n",
    "\n",
    "#                 eval_results = eval_model(val_batches, criterion)\n",
    "#                 scheduler.step(eval_results['eval_loss'])\n",
    "\n",
    "#                 print(f'Global Step {global_step} Eval loss : {eval_results[\"eval_loss\"]}, Eval accuracy : {eval_results[\"eval_accuracy\"]}')\n",
    "#                 if eval_results[\"eval_loss\"] < best_eval_loss:\n",
    "#                     best_eval_loss = eval_results['eval_loss']\n",
    "#                     # save model.state_dict()\n",
    "#                     torch.save(model.state_dict(), state_save_path)\n",
    "#                     print(f'[Saving at] {state_save_path}')\n",
    "\n",
    "            \n",
    "            global_step += 1\n",
    "            local_step += 1\n",
    "\n",
    "            #print(f'batch {idx} | Loss {loss.item()}')\n",
    "        \n",
    "        total_loss = total_loss / local_step\n",
    "        train_accuracy = accuracy / local_step\n",
    "        print(f'\\n[Epoch {epoch}] | train_cost {total_loss} | train_accuracy {train_accuracy} | time {time.time()-start_time}')\n",
    "        \n",
    "        if valset is not None:\n",
    "            val_batches = train_batch(valset, batch_size, shuffle=False)\n",
    "        eval_results = eval_model(model, val_batches, criterion, device)\n",
    "        print(f'Eval loss : {eval_results[\"eval_loss\"]}, Eval accuracy : {eval_results[\"eval_accuracy\"]}')\n",
    "        if eval_results[\"eval_loss\"] < best_eval_loss:\n",
    "            best_eval_loss = eval_results['eval_loss']\n",
    "            # save model.state_dict()\n",
    "            torch.save(model.state_dict(), state_save_path)\n",
    "#             print(f'[Saving at] {state_save_path}')\n",
    "        print('---------------------------------\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58ed9f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1] | train_cost 551.3427799224853 | train_accuracy 0.0 | time 10.59098768234253\n",
      "Eval loss : 229.72518892288207, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 2] | train_cost 100.51864422700615 | train_accuracy 0.375 | time 10.356552124023438\n",
      "Eval loss : 253.44021186828613, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 3] | train_cost 25.5765392055353 | train_accuracy 0.65 | time 10.35309624671936\n",
      "Eval loss : 272.48412556648253, Eval accuracy : 0.05\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 4] | train_cost 6.261235316135935 | train_accuracy 0.8625 | time 10.347128868103027\n",
      "Eval loss : 286.4800060272217, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 5] | train_cost 5.025358309364265 | train_accuracy 0.875 | time 10.359852313995361\n",
      "Eval loss : 279.5222639083862, Eval accuracy : 0.05\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 6] | train_cost 3.843585693819249 | train_accuracy 0.9375 | time 10.374258279800415\n",
      "Eval loss : 294.6866586387157, Eval accuracy : 0.05\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 7] | train_cost 0.22787321795294702 | train_accuracy 0.975 | time 10.371108531951904\n",
      "Eval loss : 254.65999378561975, Eval accuracy : 0.05\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 8] | train_cost 2.9048858642578126 | train_accuracy 0.9625 | time 10.385095834732056\n",
      "Eval loss : 300.32571721076965, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 9] | train_cost 3.9179897829890242 | train_accuracy 0.925 | time 10.398679971694946\n",
      "Eval loss : 292.08135900199414, Eval accuracy : 0.05\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 10] | train_cost 5.568864268064499 | train_accuracy 0.925 | time 10.388375043869019\n",
      "Eval loss : 288.28420699495763, Eval accuracy : 0.1\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 11] | train_cost 3.8427295192824205 | train_accuracy 0.9375 | time 10.41203761100769\n",
      "Eval loss : 239.63323822021485, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 12] | train_cost 2.6589964388984697 | train_accuracy 0.975 | time 10.409382104873657\n",
      "Eval loss : 290.92732696533204, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 13] | train_cost 0.5784526832714615 | train_accuracy 0.975 | time 10.425829410552979\n",
      "Eval loss : 278.12425231933594, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 14] | train_cost 0.619140625 | train_accuracy 0.975 | time 10.422598361968994\n",
      "Eval loss : 272.82845344543455, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 15] | train_cost 6.604533046472352 | train_accuracy 0.9625 | time 10.42317247390747\n",
      "Eval loss : 306.99547882080077, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 16] | train_cost 0.008779308944940566 | train_accuracy 0.9875 | time 10.42302656173706\n",
      "Eval loss : 306.06139907836916, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 17] | train_cost 0.0 | train_accuracy 1.0 | time 10.422151327133179\n",
      "Eval loss : 305.4999378204346, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 18] | train_cost 0.0 | train_accuracy 1.0 | time 10.417201042175293\n",
      "Eval loss : 305.49365119934083, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 19] | train_cost 0.0 | train_accuracy 1.0 | time 10.426502466201782\n",
      "Eval loss : 305.48665657043455, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 20] | train_cost 0.0 | train_accuracy 1.0 | time 10.429924726486206\n",
      "Eval loss : 305.47993049621584, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 21] | train_cost 0.0 | train_accuracy 1.0 | time 10.429582118988037\n",
      "Eval loss : 305.4730823516846, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 22] | train_cost 0.0 | train_accuracy 1.0 | time 10.433684349060059\n",
      "Eval loss : 305.4663196563721, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 23] | train_cost 0.0 | train_accuracy 1.0 | time 10.4298677444458\n",
      "Eval loss : 305.45972175598143, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 24] | train_cost 0.0 | train_accuracy 1.0 | time 10.440359592437744\n",
      "Eval loss : 305.45286140441897, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 25] | train_cost 0.0 | train_accuracy 1.0 | time 10.445807218551636\n",
      "Eval loss : 305.4454521179199, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 26] | train_cost 0.0 | train_accuracy 1.0 | time 10.424508333206177\n",
      "Eval loss : 305.4388114929199, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 27] | train_cost 0.0 | train_accuracy 1.0 | time 10.4313325881958\n",
      "Eval loss : 305.4309257507324, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 28] | train_cost 0.0 | train_accuracy 1.0 | time 10.431405067443848\n",
      "Eval loss : 305.4229362487793, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 29] | train_cost 0.0 | train_accuracy 1.0 | time 10.436219930648804\n",
      "Eval loss : 305.4139274597168, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 30] | train_cost 0.0 | train_accuracy 1.0 | time 10.434763193130493\n",
      "Eval loss : 305.4037223815918, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 31] | train_cost 0.0 | train_accuracy 1.0 | time 10.449893951416016\n",
      "Eval loss : 305.39313278198244, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 32] | train_cost 0.0 | train_accuracy 1.0 | time 10.433658123016357\n",
      "Eval loss : 305.38198165893556, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 33] | train_cost 0.0 | train_accuracy 1.0 | time 10.432047367095947\n",
      "Eval loss : 305.3692436218262, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 34] | train_cost 0.0 | train_accuracy 1.0 | time 10.426556587219238\n",
      "Eval loss : 305.35664596557615, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 35] | train_cost 0.0 | train_accuracy 1.0 | time 10.447121381759644\n",
      "Eval loss : 305.3430290222168, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 36] | train_cost 0.0 | train_accuracy 1.0 | time 10.43453311920166\n",
      "Eval loss : 305.3284599304199, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 37] | train_cost 0.0 | train_accuracy 1.0 | time 10.431483507156372\n",
      "Eval loss : 305.3133415222168, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 38] | train_cost 0.0 | train_accuracy 1.0 | time 10.432239770889282\n",
      "Eval loss : 305.29775924682616, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 39] | train_cost 0.0 | train_accuracy 1.0 | time 10.442821025848389\n",
      "Eval loss : 305.28152389526366, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 40] | train_cost 0.0 | train_accuracy 1.0 | time 10.434042930603027\n",
      "Eval loss : 305.2650260925293, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 41] | train_cost 0.0 | train_accuracy 1.0 | time 10.432370662689209\n",
      "Eval loss : 305.2477226257324, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 42] | train_cost 0.0 | train_accuracy 1.0 | time 10.449291944503784\n",
      "Eval loss : 305.22983322143557, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 43] | train_cost 0.0 | train_accuracy 1.0 | time 10.430934429168701\n",
      "Eval loss : 305.21080856323243, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 44] | train_cost 0.0 | train_accuracy 1.0 | time 10.436357021331787\n",
      "Eval loss : 305.1911796569824, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 45] | train_cost 0.0 | train_accuracy 1.0 | time 10.439822673797607\n",
      "Eval loss : 305.17175216674804, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 46] | train_cost 0.0 | train_accuracy 1.0 | time 10.432801961898804\n",
      "Eval loss : 305.15026779174804, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 47] | train_cost 0.0 | train_accuracy 1.0 | time 10.434381484985352\n",
      "Eval loss : 305.1274223327637, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 48] | train_cost 0.0 | train_accuracy 1.0 | time 10.436949968338013\n",
      "Eval loss : 305.1038139343262, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 49] | train_cost 0.0 | train_accuracy 1.0 | time 10.43537163734436\n",
      "Eval loss : 305.0794731140137, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 50] | train_cost 0.0 | train_accuracy 1.0 | time 10.44254207611084\n",
      "Eval loss : 305.05349044799806, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 51] | train_cost 0.0 | train_accuracy 1.0 | time 10.431551456451416\n",
      "Eval loss : 305.0271598815918, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 52] | train_cost 0.0 | train_accuracy 1.0 | time 10.43422245979309\n",
      "Eval loss : 304.99860153198244, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 53] | train_cost 0.0 | train_accuracy 1.0 | time 10.444721698760986\n",
      "Eval loss : 304.96956100463865, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 54] | train_cost 0.0 | train_accuracy 1.0 | time 10.44548487663269\n",
      "Eval loss : 304.9391227722168, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 55] | train_cost 0.0 | train_accuracy 1.0 | time 10.444405555725098\n",
      "Eval loss : 304.9072319030762, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 56] | train_cost 0.0 | train_accuracy 1.0 | time 10.454506158828735\n",
      "Eval loss : 304.87434005737305, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24349/760421980.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m train(model=model, \n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mraw_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24349/1126071421.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, raw_data, epochs, batch_size, output_dir, lr, device, betas, weight_decay, val_ratio, fp16, fp16_opt_level)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mlocal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_cs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24349/1126071421.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(data, bsz, shuffle)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcs_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RuntimeError: Expected 4-dimensional input for 4-dimensional weight [256, 1, 3, 3], but got 3-dimensional input of size [320, 40, 2800] instead\n",
    "# tdata = [now_mfcc_list.to(1), next_mfcc_list.to(1), label_list.to(1)]\n",
    "tdata = [cs, ns, labels]\n",
    "# batch_size, C, H, W = 32, 1, 28, 28\n",
    "# tdata = [torch.randn(batch_size, C, H, W).to(1),torch.randn(batch_size, C, H, W).to(1), torch.randn(batch_size).to(1)]\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100 # epoch\n",
    "batch_size=32 # batch size for training\n",
    "lr=1e-5 # learning rate\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#             optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "device='cuda:1'\n",
    "\n",
    "model = WF()\n",
    "\n",
    "train(model=model, \n",
    "      raw_data=tdata, \n",
    "      epochs=epochs, \n",
    "      batch_size=batch_size, \n",
    "      output_dir='.', \n",
    "      lr=lr,\n",
    "      device=device,\n",
    "      val_ratio=0.2, \n",
    "      fp16=False, \n",
    "      fp16_opt_level='O1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae21cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
