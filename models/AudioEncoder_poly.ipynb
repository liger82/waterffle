{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCIE449cfMOY"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "id": "s-ZWoIF4gz5O",
    "outputId": "9ef0db87-2d72-4cb7-fc22-ca8bd76e6d40",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling torch-1.7.0:\n",
      "  Would remove:\n",
      "    /usr/local/bin/convert-caffe2-to-onnx\n",
      "    /usr/local/bin/convert-onnx-to-caffe2\n",
      "    /usr/local/lib/python3.7/dist-packages/caffe2/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torch-1.7.0.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torch/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled torch-1.7.0\n",
      "Uninstalling torchtext-0.9.1:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.7/dist-packages/torchtext-0.9.1.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torchtext/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled torchtext-0.9.1\n",
      "Uninstalling torchvision-0.9.1+cu101:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.7/dist-packages/torchvision-0.9.1+cu101.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libcudart.c740f4ef.so.10.1\n",
      "    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libjpeg.ceea7512.so.62\n",
      "    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libpng16.7f72a3c5.so.16\n",
      "    /usr/local/lib/python3.7/dist-packages/torchvision.libs/libz.1328edc3.so.1\n",
      "    /usr/local/lib/python3.7/dist-packages/torchvision/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled torchvision-0.9.1+cu101\n",
      "Collecting torch==1.7.0\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (1.19.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
      "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.7.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
      "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.7.0) (1.7.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (3.7.4.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchtext torchvision\n",
    "!pip install torch==1.7.0\n",
    "!pip install torchaudio==0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUww8Ywgq7Qe"
   },
   "source": [
    "# 다 직접 구현한 버전\n",
    "\n",
    "* [https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "*[https://github.com/pytorch/fairseq/blob/c36294ea4fd35eac757f417de9668b32c57d4b3d/fairseq/modules/vggblock.py#L22](https://github.com/pytorch/fairseq/blob/c36294ea4fd35eac757f417de9668b32c57d4b3d/fairseq/modules/vggblock.py#L22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR7CfPx92t-1"
   },
   "source": [
    "## ConvolutionEncoder\n",
    "* positional embedding 대신에 conv layer를 거친 입력을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G70Xn0illUAe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_Is77ag2nxI"
   },
   "outputs": [],
   "source": [
    "# def infer_conv_output_dim(conv_op, input_dim, sample_inchannel):\n",
    "#     sample_seq_len = 200\n",
    "#     sample_bsz = 10\n",
    "#     x = torch.randn(sample_bsz, sample_inchannel, sample_seq_len, input_dim)\n",
    "#     # N x C x H x W\n",
    "#     # N: sample_bsz, C: sample_inchannel, H: sample_seq_len, W: input_dim\n",
    "#     x = conv_op(x)\n",
    "#     # N x C x H x W\n",
    "#     x = x.transpose(1, 2)\n",
    "#     # N x H x C x W\n",
    "#     bsz, seq = x.size()[:2]\n",
    "#     per_channel_dim = x.size()[3]\n",
    "#     # bsz: N, seq: H, CxW the rest\n",
    "#     return x.contiguous().view(bsz, seq, -1).size(-1), per_channel_dim\n",
    "\n",
    "'''\n",
    "Two 2-D convolutional blocks, each with two conv. layers with kernel size=3, max-pooling kernel=2. The first block has 64 feature maps while the second has 128\n",
    "'''\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                 conv_kernel_size,\n",
    "                 pooling_kernel_size,\n",
    "                #  input_dim, \n",
    "                 conv_stride=1,\n",
    "                 padding=1,\n",
    "                 layer_norm=True\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input_channels=1 : mfcc는 2d이므로 채널을 1로 봐야함.\n",
    "        # kernel size 3\n",
    "        conv_op = nn.Conv2d(in_channels=in_channels, out_channels=int(out_channels/2), kernel_size=conv_kernel_size, padding=padding)\n",
    "        self.layers.append(conv_op)\n",
    "        if layer_norm:\n",
    "            self.layers.append(nn.LayerNorm(out_channels))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        conv_op = nn.Conv2d(in_channels=int(out_channels/2), out_channels=out_channels, kernel_size=conv_kernel_size, padding=padding)\n",
    "        self.layers.append(conv_op)\n",
    "        if layer_norm:\n",
    "            self.layers.append(nn.LayerNorm(out_channels))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        # max-pooling kernel=2\n",
    "        # ceil_mode : when True, will use ceil instead of floor to compute the output shape\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=pooling_kernel_size, ceil_mode=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, _ in enumerate(self.layers):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs8AJWy0lbjr",
    "outputId": "2b3888fd-6891-4806-fbc1-2e438b62fafe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvEncoder(in_channels=1, \n",
    "            out_channels=64,\n",
    "            conv_kernel_size=3,\n",
    "            pooling_kernel_size=2\n",
    "        #  ,input_dim=30\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIr1g17X29SM"
   },
   "source": [
    "## SelfAttentionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAr8FS5hrHVl"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJCb4PZkrV5G"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPXULKa2YXGh",
    "outputId": "30186a13-6229-41a8-bb69-64cfd1a24bcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (qkv_proj): Linear(in_features=200, out_features=1536, bias=True)\n",
       "  (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiheadAttention(input_dim=200\n",
    "                   ,embed_dim=512\n",
    "                   , num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhaL8oHfyfbe"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(2) 10 encoder transformer blocks all with transformer dim=1024, 16 heads, intermediate ReLU layer size=2048,\n",
    "'''\n",
    "class SelfAttentionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads=8, dropout=0.15):\n",
    "        super().__init__()\n",
    "        # input_dim - Dimensionality of the input\n",
    "        self.mha = MultiheadAttention(input_dim, embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layerNorm1 = nn.LayerNorm(embed_dim) # 앞선 것과 같은 dim\n",
    "        self.layerNorm2 = nn.LayerNorm(embed_dim) # 앞선 것과 같은 dim\n",
    "        \n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.mha(x)\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        x = self.layerNorm1(x)\n",
    "\n",
    "        x2 = self.fc(x)\n",
    "        x2 = self.relu(x2)\n",
    "        x2 = self.fc(x)\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        x = torch.cat((x, x2), dim=1)\n",
    "        x = self.layerNorm2(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_SKG2dkcvdZ",
    "outputId": "73d7d498-9e41-446f-d6ad-69bee3524af3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttentionEncoder(\n",
       "  (mha): MultiheadAttention(\n",
       "    (qkv_proj): Linear(in_features=200, out_features=1536, bias=True)\n",
       "    (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.15, inplace=False)\n",
       "  (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae = SelfAttentionEncoder(input_dim=200, \n",
    "                           embed_dim=512)\n",
    "sae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRKBeLrx3GV_"
   },
   "source": [
    "## AudioEncoder 최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLBAea5BJ00t"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Any, Dict, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiSRbmd6JZc3"
   },
   "outputs": [],
   "source": [
    "class BasicAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements simple/classical attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 1,\n",
    "        attn: str = 'cosine',\n",
    "        residual: bool = False,\n",
    "        get_weights: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if attn == 'cosine':\n",
    "            self.cosine = nn.CosineSimilarity(dim=dim)\n",
    "        self.attn = attn\n",
    "        self.dim = dim\n",
    "        self.get_weights = get_weights\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        xs: torch.Tensor,\n",
    "        ys: torch.Tensor,\n",
    "        mask_ys: Optional[torch.Tensor] = None,\n",
    "        values: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute attention.\n",
    "        Attend over ys with query xs to obtain weights, then apply weights to\n",
    "        values (ys if yalues is None)\n",
    "        Args:\n",
    "            xs: B x query_len x dim (queries)\n",
    "            ys: B x key_len x dim (keys)\n",
    "            mask_ys: B x key_len (mask)\n",
    "            values: B x value_len x dim (values); if None, default to ys\n",
    "        \"\"\"\n",
    "        bsz = xs.size(0)\n",
    "        y_len = ys.size(1)\n",
    "        x_len = xs.size(1)\n",
    "        if self.attn == 'cosine':\n",
    "            l1 = self.cosine(xs, ys).unsqueeze(self.dim - 1)\n",
    "        else:\n",
    "            l1 = torch.bmm(xs, ys.transpose(1, 2))\n",
    "            if self.attn == 'sqrt':\n",
    "                d_k = ys.size(-1)\n",
    "                l1 = l1 / math.sqrt(d_k)\n",
    "        if mask_ys is not None:\n",
    "            attn_mask = (mask_ys == 0).view(bsz, 1, y_len)\n",
    "            attn_mask = attn_mask.repeat(1, x_len, 1)\n",
    "            l1.masked_fill_(attn_mask, neginf(l1.dtype))\n",
    "        l2 = F.softmax(l1, dim=self.dim, dtype=torch.float).type_as(l1)\n",
    "        if values is None:\n",
    "            values = ys\n",
    "        lhs_emb = torch.bmm(l2, values)\n",
    "\n",
    "        # # add back the query\n",
    "        if self.residual:\n",
    "            lhs_emb = lhs_emb.add(xs)\n",
    "\n",
    "        if self.get_weights:\n",
    "            return lhs_emb.squeeze(self.dim - 1), l2\n",
    "        else:\n",
    "            return lhs_emb.squeeze(self.dim - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxHozUOSJvrb"
   },
   "outputs": [],
   "source": [
    "class PolyBasicAttention(BasicAttention):\n",
    "    \"\"\"\n",
    "    Override basic attention to account for edge case for polyencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, poly_type, n_codes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.poly_type = poly_type\n",
    "        self.n_codes = n_codes\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Account for accidental dimensionality reduction when num_codes is 1 and the\n",
    "        polyencoder type is 'codes'\n",
    "        \"\"\"\n",
    "        lhs_emb = super().forward(*args, **kwargs)\n",
    "        if self.poly_type == 'codes' and self.n_codes == 1 and len(lhs_emb.shape) == 2:\n",
    "            lhs_emb = lhs_emb.unsqueeze(self.dim - 1)\n",
    "        return lhs_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFqLnW9_KClS"
   },
   "outputs": [],
   "source": [
    "\"\"\"Near infinity, useful as a large penalty for scoring when inf is bad.\"\"\"\n",
    "NEAR_INF = 1e20\n",
    "NEAR_INF_FP16 = 65504\n",
    "\n",
    "def neginf(dtype: torch.dtype) -> float:\n",
    "    \"\"\"\n",
    "    Return a representable finite number near -inf for a dtype.\n",
    "    \"\"\"\n",
    "    if dtype is torch.float16:\n",
    "        return -NEAR_INF_FP16\n",
    "    else:\n",
    "        return -NEAR_INF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O1V3Nfa6Yzm"
   },
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 poly_n_codes, # m, the number of global contxt features\n",
    "                #  poly_attention_type,\n",
    "                 poly_attention_num_heads,\n",
    "                #  codes_attention_type,\n",
    "                 codes_attention_num_heads,\n",
    "                 embed_dim, \n",
    "\n",
    "                 num_conv_layers, \n",
    "                 in_channels, \n",
    "                 conv_kernel_size=3, \n",
    "                 pooling_kernel_size=2,\n",
    "                 num_att_layers=6, \n",
    "                 dropout=0.15):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "\n",
    "        self.n_codes = poly_n_codes\n",
    "        self.attention_num_heads = poly_attention_num_heads\n",
    "        self.codes_attention_num_heads = codes_attention_num_heads\n",
    "\n",
    "        # the codes\n",
    "        codes = torch.empty(self.n_codes, embed_dim)\n",
    "        codes = torch.nn.init.uniform_(codes)\n",
    "        self.codes = torch.nn.Parameter(codes)\n",
    "\n",
    "        # attention for the codes\n",
    "        self.code_attention = PolyBasicAttention(poly_type='codes', n_codes=self.n_codes, dim=2, attn='basic', get_weights=False)\n",
    "\n",
    "        # The final attention (the one that takes the candidate as key)\n",
    "        self.attention = MultiheadAttention(embed_dim, embed_dim, self.attention_num_heads)\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "\n",
    "        # two 2d-conv blocks\n",
    "        # in_channels=1, \n",
    "        # out_channels=64,\n",
    "        # conv_kernel_size=3,\n",
    "        # pooling_kernel_size=2,\n",
    "        for i in range(num_conv_layers):\n",
    "            self.encoder.append(\n",
    "                ConvEncoder(\n",
    "                    in_channels=in_channels, \n",
    "                    out_channels=embed_dim,\n",
    "                    conv_kernel_size=conv_kernel_size,\n",
    "                    pooling_kernel_size=pooling_kernel_size\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # SelfAttentionEncoder * num_att_layers\n",
    "        for i in range(num_att_layers):\n",
    "            self.encoder.append(\n",
    "                SelfAttentionEncoder(\n",
    "                    input_dim=embed_dim,\n",
    "                    embed_dim=embed_dim, \n",
    "                    num_heads=self.attention_num_heads, \n",
    "                    dropout=dropout)\n",
    "                )\n",
    "            \n",
    "    def attend(self, attention_layer, queries, keys, values, mask):\n",
    "        \"\"\"\n",
    "        Apply attention.\n",
    "        :param attention_layer:\n",
    "            nn.Module attention layer to use for the attention\n",
    "        :param queries:\n",
    "            the queries for attention\n",
    "        :param keys:\n",
    "            the keys for attention\n",
    "        :param values:\n",
    "            the values for attention\n",
    "        :param mask:\n",
    "            mask for the attention keys\n",
    "        :return:\n",
    "            the result of applying attention to the values, with weights computed\n",
    "            wrt to the queries and keys.\n",
    "        \"\"\"\n",
    "        if keys is None:\n",
    "            keys = values\n",
    "        if isinstance(attention_layer, PolyBasicAttention):\n",
    "            return attention_layer(queries, keys, mask_ys=mask, values=values)\n",
    "        elif isinstance(attention_layer, MultiHeadAttention):\n",
    "            return attention_layer(queries, keys, values, mask)[0]\n",
    "        else:\n",
    "            raise Exception('Unrecognized type of attention')\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                x_raw=None, \n",
    "                x_rep=None\n",
    "               ):\n",
    "        '''\n",
    "        encoding 과정과 그 이후 과정을 나눈 이유는 먼저 계속 사용하는 피쳐를 처리해놓고 재사용하는 과정을 거치기 위해서이다.\n",
    "        '''\n",
    "        if x_raw is not None:\n",
    "            # x_raw = [current song, next song]\n",
    "            # next: candidate\n",
    "            cs, ns = x_raw\n",
    "        \n",
    "            # cand mfcc를 conv->selfattention encoder를 거친 emb\n",
    "            cand_emb = self.encoder(nx)\n",
    "\n",
    "            # ctxt mfcc를 conv->self attention encoder를 거친 emb\n",
    "            ctxt_out = self.encoder(cs)\n",
    "            return ctxt_out, cand_emb\n",
    "        elif x_rep is not None:\n",
    "            ctxt_out, cand_emb = x_rep\n",
    "\n",
    "            # m개 만큼 context code를 반복\n",
    "            # ctxt_out 값과 code를 내적한 값들의 softmax한 벡터 (w_1,...,w_m)를 이전 레이어 결과값(ctxt_out)과 곱해서 합한다.\n",
    "            # 이 값이 m개의 global context features\n",
    "            bsz = cand_emb.size(0)\n",
    "            global_ctxts = self.attend(attention_layer=self.code_attention , \n",
    "                                       queries=self.codes.repeat(bsz, 1, 1), \n",
    "                                       keys=ctxt_out,\n",
    "                                       values=ctxt_out, \n",
    "                                       mask=None)        \n",
    "\n",
    "            # m개의 global context features를 cand_emb와 내적한 값을 softmax한 벡터를 (w_1,...,w_m)라 할 때, 이 가중치 값과 global contxt features를 곱해서 합한다.\n",
    "            # 이 값이 최종 ctxt_emb\n",
    "            ctxt_emb = self.attend(attention_layer=self.attention ,\n",
    "                                   queries=cand_emb,\n",
    "                                   keys=global_ctxts,\n",
    "                                   values=global_ctxts,\n",
    "                                   mask=None)        \n",
    "\n",
    "            # score: cand_emb와 ctxt_emb 간 cosine similarity값 (반환값)\n",
    "            scores = torch.sum(ctxt_emb * cand_emb, 2)\n",
    "            return scores\n",
    "        else:\n",
    "            raise Exception('The inputs, x_raw or x_rep are required.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8SrkyurGPu2",
    "outputId": "811c69aa-1ee5-43c7-fb6a-555c350fa3ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioEncoder(\n",
       "  (code_attention): PolyBasicAttention()\n",
       "  (attention): MultiheadAttention(\n",
       "    (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (encoder): ModuleList(\n",
       "    (0): ConvEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (1): ConvEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (2): SelfAttentionEncoder(\n",
       "      (mha): MultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.15, inplace=False)\n",
       "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): SelfAttentionEncoder(\n",
       "      (mha): MultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.15, inplace=False)\n",
       "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): SelfAttentionEncoder(\n",
       "      (mha): MultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.15, inplace=False)\n",
       "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): SelfAttentionEncoder(\n",
       "      (mha): MultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.15, inplace=False)\n",
       "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): SelfAttentionEncoder(\n",
       "      (mha): MultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.15, inplace=False)\n",
       "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): SelfAttentionEncoder(\n",
       "      (mha): MultiheadAttention(\n",
       "        (qkv_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.15, inplace=False)\n",
       "      (layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AudioEncoder(poly_n_codes=16, # m, the number of global contxt features / 16, 64, 360\n",
    "            #  poly_attention_type, # MultiHeadAttention으로 고정\n",
    "            poly_attention_num_heads=8,\n",
    "            #  codes_attention_type, # PolyBasicAttention으로 고정\n",
    "            codes_attention_num_heads=2,\n",
    "            embed_dim=512, \n",
    "            num_conv_layers=2, \n",
    "            in_channels=1, \n",
    "            conv_kernel_size=3, \n",
    "            pooling_kernel_size=2,\n",
    "            num_att_layers=6, \n",
    "            dropout=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LTjEHiv1YRC"
   },
   "source": [
    "# hyperparameters\n",
    "* [원논문](https://arxiv.org/pdf/1904.11660.pdf)\n",
    "* For model optimization, we use the AdaDelta algorithm with fixed learning rate=1.0 and gradient clipping at 10.0. \n",
    "* 80 epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2eu1tvS_2Zc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AudioEncoder_poly.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
