{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afcbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f5d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/home/super/waterffle/playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223052c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c337a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing import load_poly_encoder_dataset\n",
    "# now_mfcc_list, next_mfcc_list, label_list = load_poly_encoder_dataset(2800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5478df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsz * 10 개만 할당\n",
    "# now_mfcc_list = torch.tensor(now_mfcc_list[:320])\n",
    "# next_mfcc_list = torch.tensor(next_mfcc_list[:320])\n",
    "# label_list = torch.tensor(label_list[:320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d31fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 메모리 일부로 제한\n",
    "# torch.cuda.set_per_process_memory_fraction(0.5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2384a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efbf1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_poly_encoder_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b30c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_num = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30540cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pad_length = 938\n",
    "num_feature = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639b0d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/32000 [00:00<30:34, 17.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Get MFCC with padding and crop======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32000/32000 [01:15<00:00, 422.54it/s] \n"
     ]
    }
   ],
   "source": [
    "cs, ns, labels = load_poly_encoder_dataset(last_pad_length, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca15b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsz * 10 개만 할당\n",
    "small_size = 3200\n",
    "cs = torch.tensor(cs[:small_size])\n",
    "ns = torch.tensor(ns[:small_size])\n",
    "labels = torch.tensor(labels[:small_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42372c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = cs.view(-1, 1, last_pad_length, num_feature)\n",
    "ns = ns.view(-1, 1, last_pad_length, num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebdc9967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3200, 1, 938, 128]), torch.Size([3200, 1, 938, 128]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.size(), ns.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620b5bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 938, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4fccc",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f1a59a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbd3eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    cs_rep, ns_rep, labels = data\n",
    "    length = cs_rep.size()[0]\n",
    "    for i in range(0, length, bsz):\n",
    "        yield cs_rep[i:i+bsz], ns_rep[i:i+bsz], labels[i:i+bsz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afe01878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from itertools import repeat\n",
    "\n",
    "\"\"\"Near infinity, useful as a large penalty for scoring when inf is bad.\"\"\"\n",
    "NEAR_INF = 1e20\n",
    "NEAR_INF_FP16 = 65504\n",
    "\n",
    "def _pair(v):\n",
    "    if isinstance(v, Iterable):\n",
    "        assert len(v) == 2, \"len(v) != 2\"\n",
    "        return v\n",
    "    return tuple(repeat(v, 2))\n",
    "\n",
    "\n",
    "def infer_conv_output_dim(conv_op, input_dim, sample_inchannel):\n",
    "    sample_seq_len = 2000\n",
    "    sample_bsz = 32\n",
    "    x = torch.randn(sample_bsz, sample_inchannel, sample_seq_len, input_dim)\n",
    "    # N x C x H x W\n",
    "    # N: sample_bsz, C: sample_inchannel, H: sample_seq_len, W: input_dim\n",
    "    x = conv_op(x)\n",
    "    # N x C x H x W\n",
    "    x = x.transpose(1, 2)\n",
    "    # N x H x C x W\n",
    "    bsz, seq = x.size()[:2]\n",
    "    per_channel_dim = x.size()[3]\n",
    "    # bsz: N, seq: H, CxW the rest\n",
    "    return x.contiguous().view(bsz, seq, -1).size(-1), per_channel_dim\n",
    "\n",
    "\n",
    "'''\n",
    "Two 2-D convolutional blocks, each with two conv. layers with kernel size=3, max-pooling kernel=2. The first block has 64 feature maps while the second has 128\n",
    "'''\n",
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG motibated cnn module https://arxiv.org/pdf/1409.1556.pdf\n",
    "    code : https://github.com/pytorch/fairseq/blob/c36294ea4fd35eac757f417de9668b32c57d4b3d/fairseq/modules/vggblock.py#L38\n",
    "    Args:\n",
    "        in_channels: (int) number of input channels (typically 1)\n",
    "        out_channels: (int) number of output channels\n",
    "        conv_kernel_size: convolution channels\n",
    "        pooling_kernel_size: the size of the pooling window to take a max over\n",
    "        num_conv_layers: (int) number of convolution layers\n",
    "        input_dim: (int) input dimension\n",
    "        conv_stride: the stride of the convolving kernel.\n",
    "            Can be a single number or a tuple (sH, sW)  Default: 1\n",
    "        padding: implicit paddings on both sides of the input.\n",
    "            Can be a single number or a tuple (padH, padW). Default: None\n",
    "        layer_norm: (bool) if layer norm is going to be applied. Default: False\n",
    "    Shape:\n",
    "        Input: BxCxTxfeat, i.e. (batch_size, input_size, timesteps, features)\n",
    "        Output: BxCxTxfeat, i.e. (batch_size, input_size, timesteps, features)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                 conv_kernel_size,\n",
    "                 num_conv_layers,\n",
    "                 pooling_kernel_size,\n",
    "                 input_dim=None, \n",
    "                 conv_stride=1,\n",
    "                 padding=None,\n",
    "                 layer_norm=False\n",
    "                 ):\n",
    "        assert (\n",
    "            input_dim is not None\n",
    "        ), \"Need input_dim for LayerNorm and infer_conv_output_dim\"\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        \n",
    "        conv_kernel_size = _pair(conv_kernel_size)\n",
    "        pooling_kernel_size = _pair(pooling_kernel_size)\n",
    "        padding = (\n",
    "            tuple(e // 2 for e in conv_kernel_size)\n",
    "            if padding is None\n",
    "            else _pair(padding)\n",
    "        )\n",
    "        conv_stride = _pair(conv_stride)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input_channels=1 : mfcc는 2d이므로 채널을 1로 봐야함.\n",
    "        # kernel size 3\n",
    "        for layer in range(num_conv_layers):\n",
    "            conv_op = nn.Conv2d(\n",
    "                in_channels if layer == 0 else out_channels,\n",
    "                out_channels,\n",
    "                conv_kernel_size,\n",
    "                stride=conv_stride,\n",
    "                padding=padding,\n",
    "            )\n",
    "            self.layers.append(conv_op)\n",
    "            if layer_norm:\n",
    "                self.conv_output_dim, per_channel_dim = infer_conv_output_dim(\n",
    "                    conv_op, input_dim, in_channels if layer == 0 else out_channels\n",
    "                )\n",
    "                self.layers.append(nn.LayerNorm(per_channel_dim))\n",
    "                input_dim = per_channel_dim\n",
    "            #self.layers.append(nn.Dropout(p=0.2))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        if pooling_kernel_size is not None:\n",
    "            # ceil_mode : when True, will use ceil instead of floor to compute the output shape\n",
    "            pool_op = nn.MaxPool2d(kernel_size=pooling_kernel_size, ceil_mode=True)\n",
    "            self.layers.append(pool_op)\n",
    "            self.total_output_dim, self.output_dim = infer_conv_output_dim(\n",
    "                pool_op, input_dim, out_channels\n",
    "            )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        for i, _ in enumerate(self.layers):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements simple/classical attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 1,\n",
    "        attn: str = 'cosine',\n",
    "        residual: bool = False,\n",
    "        get_weights: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if attn == 'cosine':\n",
    "            self.cosine = nn.CosineSimilarity(dim=dim)\n",
    "        self.attn = attn\n",
    "        self.dim = dim\n",
    "        self.get_weights = get_weights\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        xs: torch.Tensor,\n",
    "        ys: torch.Tensor,\n",
    "        mask_ys: Optional[torch.Tensor] = None,\n",
    "        values: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute attention.\n",
    "        Attend over ys with query xs to obtain weights, then apply weights to\n",
    "        values (ys if yalues is None)\n",
    "        Args:\n",
    "            xs: B x query_len x dim (queries)\n",
    "            ys: B x key_len x dim (keys)\n",
    "            mask_ys: B x key_len (mask)\n",
    "            values: B x value_len x dim (values); if None, default to ys\n",
    "        \"\"\"\n",
    "        bsz = xs.size(0)\n",
    "        y_len = ys.size(1)\n",
    "        x_len = xs.size(1)\n",
    "        if self.attn == 'cosine':\n",
    "            l1 = self.cosine(xs, ys).unsqueeze(self.dim - 1)\n",
    "        else:\n",
    "            l1 = torch.bmm(xs, ys.transpose(1, 2))\n",
    "            if self.attn == 'sqrt':\n",
    "                d_k = ys.size(-1)\n",
    "                l1 = l1 / math.sqrt(d_k)\n",
    "        if mask_ys is not None:\n",
    "            attn_mask = (mask_ys == 0).view(bsz, 1, y_len)\n",
    "            attn_mask = attn_mask.repeat(1, x_len, 1)\n",
    "            l1.masked_fill_(attn_mask, neginf(l1.dtype))\n",
    "        l2 = F.softmax(l1, dim=self.dim, dtype=torch.float).type_as(l1)\n",
    "        if values is None:\n",
    "            values = ys\n",
    "        lhs_emb = torch.bmm(l2, values)\n",
    "\n",
    "        # # add back the query\n",
    "        if self.residual:\n",
    "            lhs_emb = lhs_emb.add(xs)\n",
    "        \n",
    "        res = lhs_emb.squeeze(self.dim - 1)\n",
    "        if self.get_weights:\n",
    "            return res, l2\n",
    "        else:\n",
    "            return res\n",
    "        \n",
    "\n",
    "class PolyBasicAttention(BasicAttention):\n",
    "    \"\"\"\n",
    "    Override basic attention to account for edge case for polyencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, poly_type, n_codes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.poly_type = poly_type\n",
    "        self.n_codes = n_codes\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Account for accidental dimensionality reduction when num_codes is 1 and the\n",
    "        polyencoder type is 'codes'\n",
    "        \"\"\"\n",
    "        lhs_emb = super().forward(*args, **kwargs)\n",
    "        if self.poly_type == 'codes' and self.n_codes == 1 and len(lhs_emb.shape) == 2:\n",
    "            lhs_emb = lhs_emb.unsqueeze(self.dim - 1)\n",
    "        return lhs_emb\n",
    "    \n",
    "\n",
    "def neginf(dtype: torch.dtype) -> float:\n",
    "    \"\"\"\n",
    "    Return a representable finite number near -inf for a dtype.\n",
    "    \"\"\"\n",
    "    if dtype is torch.float16:\n",
    "        return -NEAR_INF_FP16\n",
    "    else:\n",
    "        return -NEAR_INF\n",
    "    \n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    '''\n",
    "    code : https://github.com/pytorch/fairseq/blob/c36294ea4fd35eac757f417de9668b32c57d4b3d/examples/speech_recognition/models/vggtransformer.py#L271\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 poly_n_codes, # m, the number of global contxt features\n",
    "                 poly_attention_num_heads,\n",
    "                 num_att_layers,\n",
    "                 codes_attention_num_heads,\n",
    "                 embed_dim, \n",
    "                \n",
    "                 input_feat_per_channel,\n",
    "                 num_conv_block,\n",
    "                 num_conv_layers, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                 conv_kernel_size=3, \n",
    "                 pooling_kernel_size=2,\n",
    "                 layer_norm=False,\n",
    "                 dropout=0.1,\n",
    "                 reduction_type='first' # first, avg, max\n",
    "                ):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "                \n",
    "        self.in_channels = in_channels\n",
    "        self.input_dim = input_feat_per_channel\n",
    "        self.reduction_type = reduction_type\n",
    "        \n",
    "        self.conv_encoder_block = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_conv_block):\n",
    "            self.conv_encoder_block.append(\n",
    "                ConvEncoder(\n",
    "                    in_channels=in_channels, \n",
    "                    out_channels=out_channels,\n",
    "                    conv_kernel_size=conv_kernel_size,\n",
    "                    num_conv_layers=num_conv_layers,\n",
    "                    pooling_kernel_size=pooling_kernel_size,\n",
    "                    input_dim=input_feat_per_channel,\n",
    "                    layer_norm=layer_norm\n",
    "                    )\n",
    "                )\n",
    "            in_channels = out_channels\n",
    "            input_feat_per_channel = self.conv_encoder_block[-1].output_dim\n",
    "        \n",
    "        self.conv_encoder_block = nn.Sequential(*self.conv_encoder_block)\n",
    "        \n",
    "        # conv_output_dim is the output dimension of conv encoder\n",
    "        conv_output_dim = self.infer_conv_output_dim(self.in_channels, self.input_dim)\n",
    "        \n",
    "        self.n_codes = poly_n_codes\n",
    "        self.attention_num_heads = poly_attention_num_heads\n",
    "        self.codes_attention_num_heads = codes_attention_num_heads\n",
    "\n",
    "        # the codes\n",
    "        codes = torch.empty(self.n_codes, embed_dim)\n",
    "        codes = torch.nn.init.uniform_(codes)\n",
    "        self.codes = torch.nn.Parameter(codes)\n",
    "\n",
    "        # attention for the codes\n",
    "        self.code_attention = PolyBasicAttention(poly_type='codes', n_codes=self.n_codes, dim=2, attn='basic', get_weights=False)\n",
    "\n",
    "        # The final attention (the one that takes the candidate as key)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, \n",
    "                                               num_heads=self.attention_num_heads)\n",
    "                                               #dropout=dropout)\n",
    "\n",
    "        \n",
    "        self.att_encoder_block = nn.ModuleList()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # conv encoder를 거쳐나온 데이터의 dim과 embed_dim이 다를 경우 맞춰준다\n",
    "        if conv_output_dim != embed_dim:\n",
    "            self.att_encoder_block.append(nn.Linear(conv_output_dim, embed_dim))\n",
    "        \n",
    "        # SelfAttentionEncoder * num_att_layers\n",
    "        for i in range(num_att_layers):\n",
    "            self.att_encoder_block.append(\n",
    "                torch.nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                 nhead=self.attention_num_heads, \n",
    "                                                 dim_feedforward=embed_dim*4, \n",
    "                                                 dropout=dropout, \n",
    "                                                 activation='gelu')\n",
    "            )\n",
    "                            \n",
    "        self.att_encoder_block = nn.Sequential(*self.att_encoder_block)\n",
    "            \n",
    "    def attend(self, attention_layer, queries, keys, values, mask):\n",
    "        \"\"\"\n",
    "        Apply attention.\n",
    "        :param attention_layer:\n",
    "            nn.Module attention layer to use for the attention\n",
    "        :param queries:\n",
    "            the queries for attention\n",
    "        :param keys:\n",
    "            the keys for attention\n",
    "        :param values:\n",
    "            the values for attention\n",
    "        :param mask:\n",
    "            mask for the attention keys\n",
    "        :return:\n",
    "            the result of applying attention to the values, with weights computed\n",
    "            wrt to the queries and keys.\n",
    "        \"\"\"\n",
    "        if keys is None:\n",
    "            keys = values\n",
    "        if isinstance(attention_layer, PolyBasicAttention):\n",
    "            return attention_layer(queries, keys, values=values, mask_ys=mask)\n",
    "        elif isinstance(attention_layer, nn.MultiheadAttention):\n",
    "            return attention_layer(query=queries, key=keys, value=values, attn_mask=mask)[0]\n",
    "\n",
    "        else:\n",
    "            raise Exception('Unrecognized type of attention')\n",
    "\n",
    "    def encode(self, x_raw):\n",
    "            # x_raw = [current song, next song]\n",
    "            # next: candidate\n",
    "            cs, ns, label = x_raw\n",
    "\n",
    "            # padded tensor\n",
    "            # B, C, T, F\n",
    "            bsz, in_channels, max_seq_len, _ = ns.size()\n",
    "        \n",
    "            # cand mfcc를 conv encoder를 거친 emb\n",
    "            cand_emb = self.conv_encoder_block(ns)\n",
    "                        \n",
    "            bsz, _, output_seq_len, _ = cand_emb.size()\n",
    "\n",
    "            # (B, C, T, F) -> (B, T, C, F) -> (B, T, C*F)\n",
    "            cand_emb = cand_emb.transpose(1, 2)\n",
    "            cand_emb = cand_emb.contiguous().view(bsz, output_seq_len, -1)\n",
    "\n",
    "            # transformer encoder\n",
    "            cand_emb = self.att_encoder_block(cand_emb)\n",
    "            \n",
    "            # reduction : first, avg, max\n",
    "            if self.reduction_type=='first':\n",
    "                cand_emb = cand_emb[:,0,:]\n",
    "            elif self.reduction_type == 'avg':\n",
    "                cand_emb = torch.mean(cand_emb, dim=1)\n",
    "            elif self.reduction_type == 'max':\n",
    "                cand_emb = torch.max(cand_emb, dim=1).values\n",
    "            else:\n",
    "                raise KeyError('Not Registered reduction_type. Capable options : first, avg, and max')\n",
    "            #print(f'cand_emb : {cand_emb.size()}')\n",
    "            cand_emb = cand_emb.view(cand_emb.size()[0], 1, cand_emb.size()[1])\n",
    "            \n",
    "            # ctxt mfcc를 conv encoder를 거친 emb\n",
    "            ctxt_out = self.conv_encoder_block(cs)\n",
    "            b, c, t, f = ctxt_out.size()\n",
    "            \n",
    "            # (B, C, T, F) -> (B, T, C, F) -> (B, T, C*F)\n",
    "            ctxt_out = ctxt_out.transpose(1, 2)\n",
    "            ctxt_out = ctxt_out.contiguous().view(b, t, -1)\n",
    "            \n",
    "            # transformer encoder\n",
    "            ctxt_out = self.att_encoder_block(ctxt_out)\n",
    "            \n",
    "            return ctxt_out, cand_emb, label\n",
    "                \n",
    "    def forward(self, \n",
    "                x_raw=None, \n",
    "                x_rep=None\n",
    "               ):\n",
    "        '''\n",
    "        encoding 과정과 그 이후 과정을 나눈 이유는 먼저 계속 사용하는 피쳐를 처리해놓고 재사용하는 과정을 거치기 위해서이다.\n",
    "        '''\n",
    "        if x_raw is not None:\n",
    "            \n",
    "            return self.encode(x_raw)\n",
    "        elif x_rep is not None:\n",
    "            ctxt_out, cand_emb = x_rep\n",
    "\n",
    "            # m개 만큼 context code를 반복\n",
    "            # ctxt_out 값과 code를 내적한 값들의 softmax한 벡터 (w_1,...,w_m)를 이전 레이어 결과값(ctxt_out)과 곱해서 합한다.\n",
    "            # 이 값이 m개의 global context features\n",
    "            bsz = cand_emb.size(0)\n",
    "            # global_ctxts = [b, poly_m, dim]\n",
    "            global_ctxts = self.attend(attention_layer=self.code_attention , \n",
    "                                       queries=self.codes.repeat(bsz, 1, 1), \n",
    "                                       keys=ctxt_out,\n",
    "                                       values=ctxt_out, \n",
    "                                       mask=None) \n",
    "\n",
    "            global_ctxts = global_ctxts.transpose(0,1)\n",
    "            # cand_emb = [b,1,m] -> [1,b,m]\n",
    "            cand_emb = cand_emb.transpose(0,1)\n",
    "            \n",
    "            # m개의 global context features를 cand_emb와 내적한 값을 softmax한 벡터를 (w_1,...,w_m)라 할 때, 이 가중치 값과 global contxt features를 곱해서 합한다.\n",
    "            # 이 값이 최종 ctxt_emb\n",
    "            # ctxt_emb = cand_emb와 같은 shape\n",
    "            ctxt_emb = self.attend(attention_layer=self.attention ,\n",
    "                                   queries=cand_emb,\n",
    "                                   keys=global_ctxts,\n",
    "                                   values=global_ctxts,\n",
    "                                   mask=None)        \n",
    "            \n",
    "            #print('cand', cand_emb.size())\n",
    "            #print('ctxt', ctxt_emb.size())\n",
    "            \n",
    "            # score: cand_emb와 ctxt_emb 간 cosine similarity값 (반환값)\n",
    "            # scores = [1, bs]\n",
    "            scores = torch.sum(ctxt_emb * cand_emb, -1)\n",
    "            #print('score', scores.size())\n",
    "            \n",
    "            return scores\n",
    "        else:\n",
    "            raise Exception('Unsupported operation')\n",
    "    \n",
    "    def infer_conv_output_dim(self, in_channels, input_dim):\n",
    "        sample_seq_len = 200\n",
    "        sample_bsz = 10\n",
    "        x = torch.randn(sample_bsz, in_channels, sample_seq_len, input_dim)\n",
    "        for i, _ in enumerate(self.conv_encoder_block):\n",
    "            x = self.conv_encoder_block[i](x)\n",
    "        # (B, C, T, F) -> (B, T, C, F) -> (B, T, C*F)\n",
    "        x = x.transpose(1, 2)\n",
    "        mb, seq = x.size()[:2]\n",
    "        return x.contiguous().view(mb, seq, -1).size(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b2c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioEncoder(\n",
      "  (conv_encoder_block): Sequential(\n",
      "    (0): ConvEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (5): ReLU()\n",
      "        (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (5): ReLU()\n",
      "        (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (code_attention): PolyBasicAttention()\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (att_encoder_block): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AudioEncoder(\n",
    "                     poly_n_codes=64, # m, the number of global contxt features\n",
    "                     poly_attention_num_heads=8, \n",
    "                     codes_attention_num_heads=2,\n",
    "                     num_att_layers=6,\n",
    "                     embed_dim=512, \n",
    "                     input_feat_per_channel=num_feature, # feature vector dimension\n",
    "                     num_conv_block=2,\n",
    "                     num_conv_layers=2, \n",
    "                     in_channels=1, \n",
    "                     out_channels=32,\n",
    "                     conv_kernel_size=3, \n",
    "                     pooling_kernel_size=2,\n",
    "                     layer_norm=True,\n",
    "                     reduction_type='first'\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd06af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fd1fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion):\n",
    "    # layer 중 dropout layer와 같이 학습 시에는 사용하는데 inference할 때는 사용하지 않는 경우를 구분해주기 위함\n",
    "    model.eval()\n",
    "    \n",
    "    n_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    acc = 0.0\n",
    "\n",
    "    # autograd engine을 끔(gradient 계산하지 않음) --> 메모리 사용량을 줄이고 연산 속도를 높이기 위함\n",
    "    with torch.no_grad():\n",
    "        # input은 이미 1차 임베딩은 되어 있는 것을 가져왔다고 가정한다.\n",
    "        for idx, (batch_cs, batch_ns, batch_label) in enumerate(dataloader):\n",
    "            #print('batch idx', idx)\n",
    "            '''\n",
    "            n_step+=1\n",
    "            batch_labels.append(label) \n",
    "            scores.append(model(x_rep=[cs_rep, ns_rep]))\n",
    "\n",
    "            if (idx+1) % batch_size == 0:\n",
    "                mx_i = np.argmax(scores)\n",
    "                predict_batch = [0]*batch_size\n",
    "                predict_batch[mx_i] = 1\n",
    "                predicts.extend(predict_batch)\n",
    "                labels.extend(batch_labels)\n",
    "                for b in range(batch_size):\n",
    "                    if b == mx_i:\n",
    "                        loss = criterion(1, batch_labels[b])\n",
    "                    else:\n",
    "                        loss = criterion(0, batch_labels[b])\n",
    "                    \n",
    "                    eval_loss += loss\n",
    "            '''\n",
    "            n_steps+=1\n",
    "            # validation set to mfcc encoder\n",
    "            cs_rep, ns_rep, label = model(x_raw=[batch_cs, batch_ns, batch_label])\n",
    "            \n",
    "            batch_label = torch.LongTensor([torch.argmax(label).item()]).to(device_num)\n",
    "\n",
    "            score = model(x_rep=[cs_rep, ns_rep])\n",
    "\n",
    "            #print('score', score)\n",
    "            \n",
    "            # cross entropy          \n",
    "            loss = criterion(score, batch_label)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            predict = torch.argmax(score, axis=1)\n",
    "            #print(predict)\n",
    "            #print(batch_label)\n",
    "            if predict == batch_label:\n",
    "                acc += 1\n",
    "\n",
    "\n",
    "    results = {\n",
    "        'eval_accuracy': acc / n_steps,\n",
    "        'eval_loss': eval_loss / n_steps\n",
    "    }\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d2c8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, raw_data, optimizer, epochs, batch_size, output_dir, val_ratio=0.2, eval_freq=10, fp16=False, fp16_opt_level='O1'):\n",
    "    # fp16=True면 amp를 통한 mixed preicision training을 한다는 의미\n",
    "    # 사용 조건 : Volta 이상의 nvidia 그래픽 카드(v100, rtx2080ti, 등)\n",
    "    if fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=fp16_opt_level)\n",
    "    \n",
    "    def train_batch(data, bsz, shuffle=True):\n",
    "        cs_rep, ns_rep, labels = data\n",
    "        length = cs_rep.size()[0]\n",
    "        \n",
    "        for i in range(0, length, bsz):\n",
    "            cs_batch = cs_rep[i:i+bsz]\n",
    "            ns_batch = ns_rep[i:i+bsz]\n",
    "            label_batch = labels[i:i+bsz]\n",
    "            \n",
    "            if shuffle:\n",
    "                indexes = torch.randperm(bsz)              \n",
    "                yield [cs_batch[indexes], ns_batch[indexes], label_batch[indexes]]\n",
    "            else:\n",
    "                yield [cs_batch, ns_batch, label_batch]\n",
    "            #yield model(x_raw=[cs_rep[i:i+bsz], ns_rep[i:i+bsz], labels[i:i+bsz]])\n",
    "            \n",
    "    state_save_path = os.path.join(output_dir, '{}_{}_{}_pytorch_model.bin'.format('polyencoder', '64', 'dh'))\n",
    "            \n",
    "    best_eval_loss = float('inf')\n",
    "    \n",
    "    cs, ns, labels = raw_data\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 재사용 가능한 임베딩들은 미리 뽑기\n",
    "    trainset = None\n",
    "    valset = None\n",
    "    if val_ratio > 0:\n",
    "        train_len = int((1-val_ratio)*len(labels))\n",
    "        val_len = int(val_ratio*len(labels))\n",
    "        trainset = [cs[:train_len], ns[:train_len], labels[:train_len]]\n",
    "        valset = [cs[train_len:], ns[train_len:], labels[train_len:]]\n",
    "        #trainset = [cs[val_len:], ns[val_len:], labels[val_len:]]\n",
    "        #valset = [cs[:val_len], ns[:val_len], labels[:val_len]]\n",
    "        print('train', trainset[0].shape, trainset[1].shape, trainset[2].shape)\n",
    "        print('val', valset[0].shape, valset[1].shape, valset[2].shape)\n",
    "    else:\n",
    "        trainset = raw_data\n",
    "    \n",
    "    batch_labels = []\n",
    "    scores = []\n",
    "    global_step = 0\n",
    "    #TODO : 나중에 tqdm으로 바꾸기\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.zero_grad()\n",
    "        start_time = time.time()\n",
    "        train_batches = train_batch(trainset, batch_size)\n",
    "            \n",
    "        total_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        local_step = 0\n",
    "        # batch == 1set (data 32)\n",
    "        for idx, (batch_cs, batch_ns, batch_label) in enumerate(train_batches):\n",
    "            model.train()\n",
    "            local_step += 1\n",
    "            # mfcc embedding encoder\n",
    "            cs_rep, ns_rep, label = model(x_raw=[batch_cs, batch_ns, batch_label])\n",
    "\n",
    "            # label = [32]\n",
    "            #label = torch.argmax(label).item().long()\n",
    "            batch_label = torch.LongTensor([torch.argmax(label).item()]).to(device_num)\n",
    "\n",
    "            # poly encoder\n",
    "            # scores = [1,32] \n",
    "            scores = model(x_rep=[cs_rep, ns_rep])\n",
    "            #print('scores', scores)\n",
    "            \n",
    "            # cross entropy          \n",
    "            loss = criterion(scores, batch_label)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predict = torch.argmax(scores, axis=1)\n",
    "            #print(predict)\n",
    "            #print(batch_label)\n",
    "            if predict == batch_label:\n",
    "                accuracy += 1\n",
    "\n",
    "            '''\n",
    "            # Scheduling\n",
    "            if global_step and global_step % eval_freq == 0:\n",
    "                print('score', scores)\n",
    "                if valset is not None:\n",
    "                    val_batches = train_batch(valset, batch_size)\n",
    "                eval_results = eval_model(model, val_batches, criterion)\n",
    "                # torch.optim.lr_scheduler.ReduceLROnPlateau는 val_loss를 입력으로 받음\n",
    "                # 성능 향상이 없을 때 학습율을 낮춤.\n",
    "                scheduler.step(eval_results['eval_loss'])\n",
    "                print(f'Global Step {global_step} Eval loss : {eval_results[\"eval_loss\"]}, Eval accuracy : {eval_results[\"eval_accuracy\"]}')\n",
    "                if eval_results[\"eval_loss\"] < best_eval_loss:\n",
    "                    best_eval_loss = eval_results['eval_loss']\n",
    "                    torch.save(model.state_dict(), state_save_path)\n",
    "                \n",
    "            global_step += 1\n",
    "            '''\n",
    "        total_loss = total_loss / local_step\n",
    "        accuracy = accuracy / local_step\n",
    "        with open('log_32000_210903.txt', 'w') as f:\n",
    "            print(f'\\n[Epoch {epoch}] | total_Loss {total_loss} | accuracy {accuracy} | time {time.time()-start_time}\\n')\n",
    "            f.write(f'\\n[Epoch {epoch}] | total_Loss {total_loss} | accuracy {accuracy} | time {time.time()-start_time}\\n')\n",
    "        \n",
    "            if valset is not None:\n",
    "                val_batches = train_batch(valset, batch_size, shuffle=True)\n",
    "                eval_results = eval_model(model, val_batches, criterion)\n",
    "                print(f'Eval loss : {eval_results[\"eval_loss\"]}, Eval accuracy : {eval_results[\"eval_accuracy\"]}')\n",
    "                f.write(f'Eval loss : {eval_results[\"eval_loss\"]}, Eval accuracy : {eval_results[\"eval_accuracy\"]}')\n",
    "\n",
    "#         if eval_results[\"eval_loss\"] < best_eval_loss:\n",
    "#             best_eval_loss = eval_results['eval_loss']\n",
    "#             # save model.state_dict()\n",
    "#             #torch.save(model.state_dict(), state_save_path)\n",
    "# #             print(f'[Saving at] {state_save_path}')\n",
    "        print('---------------------------------\\n')\n",
    "    \n",
    "        '''\n",
    "            # eval로 scheduler를 조절해야 하는가?\n",
    "            #scheduler.step()\n",
    "            # 해줘야 하나\n",
    "            #model.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            eval_results = eval_model(val_batches)\n",
    "            \n",
    "            predict = torch.argmax(scores)\n",
    "            if predict == 0:\n",
    "                print('OK')\n",
    "                accuracy += 1\n",
    "            \n",
    "            if eval_results['eval_loss'] < best_eval_loss:\n",
    "                # save model.state_dict()\n",
    "                torch.save(model.state_dict(), state_save_path)\n",
    "                logger.info(f'Epoch {epoch} | Loss {loss.item()} | [Saving at] {state_save_path}')\n",
    "        '''\n",
    "                \n",
    "\n",
    "        # save model.state_dict()\n",
    "#         torch.save(model.state_dict(), state_save_path)\n",
    "#         logger.info(f'Epoch {epoch} | Loss {loss.item()} | [Saving at] {state_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "629b4705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train torch.Size([2560, 1, 938, 128]) torch.Size([2560, 1, 938, 128]) torch.Size([2560])\n",
      "val torch.Size([640, 1, 938, 128]) torch.Size([640, 1, 938, 128]) torch.Size([640])\n",
      "\n",
      "[Epoch 1] | total_Loss 6.421015283465385 | accuracy 0.0625 | time 31.40020728111267\n",
      "\n",
      "Eval loss : 3.67012619972229, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 2] | total_Loss 4.931183034740388 | accuracy 0.05 | time 31.41671371459961\n",
      "\n",
      "Eval loss : 3.6161760807037355, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 3] | total_Loss 4.508502745628357 | accuracy 0.025 | time 31.405723810195923\n",
      "\n",
      "Eval loss : 3.621930170059204, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 4] | total_Loss 4.010581113398075 | accuracy 0.0875 | time 31.424458503723145\n",
      "\n",
      "Eval loss : 3.759340834617615, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 5] | total_Loss 3.741730653261766 | accuracy 0.0875 | time 31.43504524230957\n",
      "\n",
      "Eval loss : 4.400401079654694, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 6] | total_Loss 3.943715394474566 | accuracy 0.1125 | time 31.443582773208618\n",
      "\n",
      "Eval loss : 4.678189826011658, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 7] | total_Loss 3.5597551606595514 | accuracy 0.1 | time 31.44765853881836\n",
      "\n",
      "Eval loss : 4.636325144767762, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 8] | total_Loss 3.1445507649332285 | accuracy 0.1625 | time 31.446473836898804\n",
      "\n",
      "Eval loss : 5.246213161945343, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 9] | total_Loss 3.1980689389631154 | accuracy 0.175 | time 31.44908332824707\n",
      "\n",
      "Eval loss : 6.816773748397827, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 10] | total_Loss 3.1365771615877747 | accuracy 0.15 | time 31.446691751480103\n",
      "\n",
      "Eval loss : 8.609383964538575, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 11] | total_Loss 3.0538792938459665 | accuracy 0.1625 | time 31.45329213142395\n",
      "\n",
      "Eval loss : 7.95779128074646, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 12] | total_Loss 2.735471651982516 | accuracy 0.2625 | time 31.448004007339478\n",
      "\n",
      "Eval loss : 6.6630380868911745, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 13] | total_Loss 2.68663210067898 | accuracy 0.2625 | time 31.450915813446045\n",
      "\n",
      "Eval loss : 7.872829270362854, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 14] | total_Loss 2.648511715978384 | accuracy 0.225 | time 31.449023246765137\n",
      "\n",
      "Eval loss : 11.666536021232606, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 15] | total_Loss 2.864225696772337 | accuracy 0.225 | time 31.447241067886353\n",
      "\n",
      "Eval loss : 11.385866904258728, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 16] | total_Loss 2.515420311642811 | accuracy 0.2125 | time 31.450088024139404\n",
      "\n",
      "Eval loss : 8.877392482757568, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 17] | total_Loss 2.5537414639256895 | accuracy 0.25 | time 31.447239875793457\n",
      "\n",
      "Eval loss : 11.11310317516327, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 18] | total_Loss 2.8238418206805362 | accuracy 0.1625 | time 31.452431201934814\n",
      "\n",
      "Eval loss : 10.377103447914124, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 19] | total_Loss 2.4098873391747473 | accuracy 0.2625 | time 31.451685428619385\n",
      "\n",
      "Eval loss : 12.62541093826294, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 20] | total_Loss 2.697206543199718 | accuracy 0.175 | time 31.450027465820312\n",
      "\n",
      "Eval loss : 9.557771301269531, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 21] | total_Loss 2.4154909323900937 | accuracy 0.2 | time 31.448187828063965\n",
      "\n",
      "Eval loss : 13.103527545928955, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 22] | total_Loss 2.2796482272446155 | accuracy 0.25 | time 31.451178073883057\n",
      "\n",
      "Eval loss : 16.78334848880768, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 23] | total_Loss 2.475622204504907 | accuracy 0.25 | time 31.450692176818848\n",
      "\n",
      "Eval loss : 13.089407467842102, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 24] | total_Loss 2.1911833790130912 | accuracy 0.275 | time 31.449488639831543\n",
      "\n",
      "Eval loss : 12.371109437942504, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 25] | total_Loss 2.453571523168648 | accuracy 0.3875 | time 31.44994282722473\n",
      "\n",
      "Eval loss : 4.305735731124878, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 26] | total_Loss 3.412455859592592 | accuracy 0.175 | time 31.451438188552856\n",
      "\n",
      "Eval loss : 5.9805765628814695, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 27] | total_Loss 3.2977113294880835 | accuracy 0.15 | time 31.450564861297607\n",
      "\n",
      "Eval loss : 6.394392502307892, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 28] | total_Loss 2.4836697699560317 | accuracy 0.3125 | time 31.44852113723755\n",
      "\n",
      "Eval loss : 7.473902559280395, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 29] | total_Loss 2.0844433354679497 | accuracy 0.325 | time 31.451159238815308\n",
      "\n",
      "Eval loss : 12.43446786403656, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 30] | total_Loss 1.9742388370366775 | accuracy 0.475 | time 31.449830770492554\n",
      "\n",
      "Eval loss : 3.608952724933624, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 31] | total_Loss 2.335724390934047 | accuracy 0.375 | time 31.448631763458252\n",
      "\n",
      "Eval loss : 14.661855125427246, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 32] | total_Loss 1.551855277435699 | accuracy 0.6125 | time 31.45373296737671\n",
      "\n",
      "Eval loss : 5.824739027023315, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 33] | total_Loss 1.5666587031720838 | accuracy 0.5 | time 31.44749665260315\n",
      "\n",
      "Eval loss : 24.018882846832277, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 34] | total_Loss 0.5231921681203126 | accuracy 0.8625 | time 31.44437050819397\n",
      "\n",
      "Eval loss : 19.69373288154602, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 35] | total_Loss 0.9553620887803482 | accuracy 0.775 | time 31.446683883666992\n",
      "\n",
      "Eval loss : 32.99547436237335, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 36] | total_Loss 1.0184012511603335 | accuracy 0.75 | time 31.44438910484314\n",
      "\n",
      "Eval loss : 7.271817207336426, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 37] | total_Loss 0.34301853837887675 | accuracy 0.9125 | time 31.448925733566284\n",
      "\n",
      "Eval loss : 20.72077579498291, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 38] | total_Loss 0.7762719602469468 | accuracy 0.7875 | time 31.448139190673828\n",
      "\n",
      "Eval loss : 15.821637678146363, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 39] | total_Loss 1.4874330251066936 | accuracy 0.6125 | time 31.447164297103882\n",
      "\n",
      "Eval loss : 22.58793032169342, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 40] | total_Loss 0.5692934674730996 | accuracy 0.9 | time 31.45162296295166\n",
      "\n",
      "Eval loss : 13.232623839378357, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 41] | total_Loss 0.4366671196090534 | accuracy 0.875 | time 31.447409868240356\n",
      "\n",
      "Eval loss : 18.566668593883513, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 42] | total_Loss 0.2848054663349166 | accuracy 0.9375 | time 31.448885202407837\n",
      "\n",
      "Eval loss : 30.397927618026735, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 43] | total_Loss 0.1692743923404519 | accuracy 0.975 | time 31.449556350708008\n",
      "\n",
      "Eval loss : 21.52620220184326, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 44] | total_Loss 0.028308732626163645 | accuracy 0.9875 | time 31.44813847541809\n",
      "\n",
      "Eval loss : 29.6148934841156, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 45] | total_Loss 0.2813337883445531 | accuracy 0.9625 | time 31.448664903640747\n",
      "\n",
      "Eval loss : 10.080939793586731, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 46] | total_Loss 0.0774065877185639 | accuracy 0.975 | time 31.446113348007202\n",
      "\n",
      "Eval loss : 33.83803868293762, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 47] | total_Loss 0.5732558278996135 | accuracy 0.875 | time 31.44912624359131\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss : 29.2113347530365, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 48] | total_Loss 0.362681138484383 | accuracy 0.9125 | time 31.447262048721313\n",
      "\n",
      "Eval loss : 25.047244119644166, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 49] | total_Loss 0.3481323160866814 | accuracy 0.9375 | time 31.447887182235718\n",
      "\n",
      "Eval loss : 16.748266649246215, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 50] | total_Loss 0.4836579773665365 | accuracy 0.9 | time 31.447824478149414\n",
      "\n",
      "Eval loss : 14.943757912516594, Eval accuracy : 0.05\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 51] | total_Loss 0.095834329180337 | accuracy 0.9625 | time 31.449638843536377\n",
      "\n",
      "Eval loss : 29.85351159572601, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 52] | total_Loss 0.20720743866793345 | accuracy 0.95 | time 31.45248532295227\n",
      "\n",
      "Eval loss : 8.283193755149842, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 53] | total_Loss 0.0833971527784823 | accuracy 0.9875 | time 31.450571060180664\n",
      "\n",
      "Eval loss : 36.345181131362914, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 54] | total_Loss 0.26687770821173507 | accuracy 0.925 | time 31.447319507598877\n",
      "\n",
      "Eval loss : 24.838129234313964, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 55] | total_Loss 0.2709177878179235 | accuracy 0.95 | time 31.451046228408813\n",
      "\n",
      "Eval loss : 12.134321236610413, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 56] | total_Loss 0.04883728260008091 | accuracy 0.975 | time 31.445618867874146\n",
      "\n",
      "Eval loss : 44.04648399353027, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 57] | total_Loss 0.09209395099465531 | accuracy 0.9625 | time 31.44474506378174\n",
      "\n",
      "Eval loss : 28.602408504486085, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 58] | total_Loss 0.00988766140692574 | accuracy 0.9875 | time 31.450716972351074\n",
      "\n",
      "Eval loss : 24.15012836456299, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 59] | total_Loss 0.13285540611448834 | accuracy 0.9625 | time 31.45086979866028\n",
      "\n",
      "Eval loss : 16.1056339263916, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 60] | total_Loss 0.4449214180670059 | accuracy 0.9125 | time 31.450379610061646\n",
      "\n",
      "Eval loss : 31.774374389648436, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 61] | total_Loss 0.014553837615016985 | accuracy 1.0 | time 31.446203231811523\n",
      "\n",
      "Eval loss : 47.65473556518555, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 62] | total_Loss 2.041447685563469e-07 | accuracy 1.0 | time 31.438753604888916\n",
      "\n",
      "Eval loss : 54.03504495620727, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 63] | total_Loss 0.7201632079587121 | accuracy 0.9125 | time 31.441894054412842\n",
      "\n",
      "Eval loss : 3.5359350442886353, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 64] | total_Loss 1.968995117478844 | accuracy 0.675 | time 31.447362899780273\n",
      "\n",
      "Eval loss : 14.218509483337403, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 65] | total_Loss 0.5402303557644358 | accuracy 0.9 | time 31.44514536857605\n",
      "\n",
      "Eval loss : 7.1681112289428714, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 66] | total_Loss 0.6187202971330716 | accuracy 0.8875 | time 31.450586557388306\n",
      "\n",
      "Eval loss : 18.4267644405365, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 67] | total_Loss 0.044167774215438715 | accuracy 0.9875 | time 31.44742703437805\n",
      "\n",
      "Eval loss : 20.12150020599365, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 68] | total_Loss 0.4736007693026191 | accuracy 0.9125 | time 31.449936628341675\n",
      "\n",
      "Eval loss : 46.141600894927976, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 69] | total_Loss 0.00047230922558396673 | accuracy 1.0 | time 31.445494890213013\n",
      "\n",
      "Eval loss : 39.214414501190184, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 70] | total_Loss 0.4113651304555333 | accuracy 0.9 | time 31.448244094848633\n",
      "\n",
      "Eval loss : 18.310910320281984, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 71] | total_Loss 0.38312205105222147 | accuracy 0.9625 | time 31.447874546051025\n",
      "\n",
      "Eval loss : 22.213243818283082, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 72] | total_Loss 0.08985723708195366 | accuracy 0.975 | time 31.447959899902344\n",
      "\n",
      "Eval loss : 32.24152841567993, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 73] | total_Loss 0.007653118405061577 | accuracy 1.0 | time 31.44863986968994\n",
      "\n",
      "Eval loss : 44.888700771331784, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 74] | total_Loss 2.477573108867759e-05 | accuracy 1.0 | time 31.44684863090515\n",
      "\n",
      "Eval loss : 44.93782081604004, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 75] | total_Loss 0.0018051512538585612 | accuracy 1.0 | time 31.446563959121704\n",
      "\n",
      "Eval loss : 40.333920288085935, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 76] | total_Loss 0.0 | accuracy 1.0 | time 31.44711995124817\n",
      "\n",
      "Eval loss : 40.30858755111694, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 77] | total_Loss 0.24612978555246015 | accuracy 0.95 | time 31.447751998901367\n",
      "\n",
      "Eval loss : 24.117954444885253, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 78] | total_Loss 0.3920133678565126 | accuracy 0.925 | time 31.444682598114014\n",
      "\n",
      "Eval loss : 21.46118016242981, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 79] | total_Loss 0.3656156941373336 | accuracy 0.9375 | time 31.444191455841064\n",
      "\n",
      "Eval loss : 43.207895851135255, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 80] | total_Loss 0.14283259994783676 | accuracy 0.95 | time 31.446396350860596\n",
      "\n",
      "Eval loss : 42.24122066497803, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 81] | total_Loss 0.821502994592902 | accuracy 0.875 | time 31.449122428894043\n",
      "\n",
      "Eval loss : 18.109407806396483, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 82] | total_Loss 0.41805652034071494 | accuracy 0.925 | time 31.44598937034607\n",
      "\n",
      "Eval loss : 27.7432852268219, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 83] | total_Loss 0.33330660329914447 | accuracy 0.95 | time 31.454012870788574\n",
      "\n",
      "Eval loss : 16.668658924102782, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 84] | total_Loss 0.44303601275225074 | accuracy 0.875 | time 31.447327613830566\n",
      "\n",
      "Eval loss : 30.56835870742798, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 85] | total_Loss 0.43004882210809603 | accuracy 0.9125 | time 31.447685718536377\n",
      "\n",
      "Eval loss : 54.31711311340332, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 86] | total_Loss 0.006073395881907029 | accuracy 1.0 | time 31.444999933242798\n",
      "\n",
      "Eval loss : 53.581084966659546, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 87] | total_Loss 0.00024373680768112748 | accuracy 1.0 | time 31.440906524658203\n",
      "\n",
      "Eval loss : 55.0723301410675, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 88] | total_Loss 1.6813568737106265e-05 | accuracy 1.0 | time 31.44362163543701\n",
      "\n",
      "Eval loss : 54.80555200576782, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 89] | total_Loss 0.9013089187320655 | accuracy 0.8625 | time 31.4437096118927\n",
      "\n",
      "Eval loss : 41.556626892089845, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 90] | total_Loss 0.23989655011709807 | accuracy 0.9625 | time 31.445393800735474\n",
      "\n",
      "Eval loss : 16.92128989696503, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 91] | total_Loss 0.3345410498867187 | accuracy 0.925 | time 31.444501638412476\n",
      "\n",
      "Eval loss : 21.9420072555542, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 92] | total_Loss 0.07914397562115197 | accuracy 0.975 | time 31.448240041732788\n",
      "\n",
      "Eval loss : 46.01822052001953, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 93] | total_Loss 0.14364786270231225 | accuracy 0.95 | time 31.44485378265381\n",
      "\n",
      "Eval loss : 50.282869529724124, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 94] | total_Loss 0.0347253092684209 | accuracy 0.975 | time 31.443320989608765\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss : 78.74414196014405, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 95] | total_Loss 0.6860778550336999 | accuracy 0.925 | time 31.448648691177368\n",
      "\n",
      "Eval loss : 30.604440021514893, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 96] | total_Loss 0.7822447508747633 | accuracy 0.8875 | time 31.442970275878906\n",
      "\n",
      "Eval loss : 76.2183967590332, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 97] | total_Loss 0.13747063485123245 | accuracy 0.975 | time 31.440403938293457\n",
      "\n",
      "Eval loss : 80.17519359588623, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 98] | total_Loss 0.14170609021100794 | accuracy 0.9625 | time 31.44185447692871\n",
      "\n",
      "Eval loss : 75.18725085258484, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 99] | total_Loss 0.03651561323729435 | accuracy 0.975 | time 31.444196462631226\n",
      "\n",
      "Eval loss : 76.35005462169647, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "[Epoch 100] | total_Loss 0.009493711554204864 | accuracy 0.9875 | time 31.4379985332489\n",
      "\n",
      "Eval loss : 87.89640531539916, Eval accuracy : 0.0\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RuntimeError: Expected 4-dimensional input for 4-dimensional weight [256, 1, 3, 3], but got 3-dimensional input of size [320, 40, 2800] instead\n",
    "# tdata = [now_mfcc_list.to(1), next_mfcc_list.to(1), label_list.to(1)]\n",
    "tdata = [cs.to(device_num), ns.to(device_num), labels.to(device_num)]\n",
    "# batch_size, C, H, W = 32, 1, 28, 28\n",
    "# tdata = [torch.randn(batch_size, C, H, W).to(1),torch.randn(batch_size, C, H, W).to(1), torch.randn(batch_size).to(1)]\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100 # epoch\n",
    "batch_size=32 # batch size for training\n",
    "lr=5e-5 # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.1, threshold=0.01, patience=5)\n",
    "\n",
    "train(model=model.to(device_num), \n",
    "      raw_data=tdata, \n",
    "      optimizer=optimizer, \n",
    "      epochs=epochs, \n",
    "      batch_size=batch_size,\n",
    "      output_dir='.',\n",
    "      val_ratio=0.2, \n",
    "      eval_freq=10,\n",
    "      fp16=False, \n",
    "      fp16_opt_level='O1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072292ab",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b399ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 12 15:08:14 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 30%   39C    P8    33W / 350W |     23MiB / 24268MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\r\n",
      "| 30%   35C    P8    28W / 350W |      5MiB / 24268MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1184      G   /usr/lib/xorg/Xorg                  9MiB |\r\n",
      "|    0   N/A  N/A      1470      G   /usr/bin/gnome-shell               12MiB |\r\n",
      "|    1   N/A  N/A      1184      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a5a32bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7460,  0.4928,  0.2707, -1.0255, -1.4475, -0.3103,  1.4932,  1.5739,\n",
      "          0.8782, -0.8915]])\n",
      "tensor([[0.6783, 0.6208, 0.5673, 0.2640, 0.1904, 0.4230, 0.8166, 0.8283, 0.7065,\n",
      "         0.2908]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,10)\n",
    "m = nn.Sigmoid()\n",
    "print(a)\n",
    "print(m(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
