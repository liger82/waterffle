{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afcbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f5d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/home/super/waterffle/playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223052c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c337a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing import load_poly_encoder_dataset\n",
    "# now_mfcc_list, next_mfcc_list, label_list = load_poly_encoder_dataset(2800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5478df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsz * 10 개만 할당\n",
    "# now_mfcc_list = torch.tensor(now_mfcc_list[:320])\n",
    "# next_mfcc_list = torch.tensor(next_mfcc_list[:320])\n",
    "# label_list = torch.tensor(label_list[:320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d31fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 메모리 일부로 제한\n",
    "# torch.cuda.set_per_process_memory_fraction(0.5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2384a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efbf1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_poly_encoder_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b30c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30540cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pad_length = 2800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "639b0d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 132/32000 [00:00<00:24, 1312.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Get MFCC with padding and crop======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32000/32000 [00:23<00:00, 1377.44it/s]\n"
     ]
    }
   ],
   "source": [
    "cs, ns, labels = load_poly_encoder_dataset(last_pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca15b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsz * 10 개만 할당\n",
    "cs = torch.tensor(cs[:320])\n",
    "ns = torch.tensor(ns[:320])\n",
    "labels = torch.tensor(labels[:320])\n",
    "\n",
    "# cs = torch.tensor(cs[:3200])\n",
    "# ns = torch.tensor(ns[:3200])\n",
    "# labels = torch.tensor(labels[:3200])\n",
    "# cs = torch.tensor(cs)\n",
    "# ns = torch.tensor(ns)\n",
    "# labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42372c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = cs.view(-1, 1, last_pad_length, 40)\n",
    "ns = ns.view(-1, 1, last_pad_length, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebdc9967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([320, 1, 2800, 40]), torch.Size([320, 1, 2800, 40]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.size(), ns.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620b5bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2800, 40])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4fccc",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1a59a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd3eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    cs_rep, ns_rep, labels = data\n",
    "    length = cs_rep.size()[0]\n",
    "    for i in range(0, length, bsz):\n",
    "        yield cs_rep[i:i+bsz], ns_rep[i:i+bsz], labels[i:i+bsz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afe01878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from itertools import repeat\n",
    "\n",
    "\"\"\"Near infinity, useful as a large penalty for scoring when inf is bad.\"\"\"\n",
    "NEAR_INF = 1e20\n",
    "NEAR_INF_FP16 = 65504\n",
    "\n",
    "def _pair(v):\n",
    "    if isinstance(v, Iterable):\n",
    "        assert len(v) == 2, \"len(v) != 2\"\n",
    "        return v\n",
    "    return tuple(repeat(v, 2))\n",
    "\n",
    "\n",
    "def infer_conv_output_dim(conv_op, input_dim, sample_inchannel):\n",
    "    sample_seq_len = 200\n",
    "    sample_bsz = 10\n",
    "    x = torch.randn(sample_bsz, sample_inchannel, sample_seq_len, input_dim)\n",
    "    # N x C x H x W\n",
    "    # N: sample_bsz, C: sample_inchannel, H: sample_seq_len, W: input_dim\n",
    "    x = conv_op(x)\n",
    "    # N x C x H x W\n",
    "    x = x.transpose(1, 2)\n",
    "    # N x H x C x W\n",
    "    bsz, seq = x.size()[:2]\n",
    "    per_channel_dim = x.size()[3]\n",
    "    # bsz: N, seq: H, CxW the rest\n",
    "    return x.contiguous().view(bsz, seq, -1).size(-1), per_channel_dim\n",
    "\n",
    "\n",
    "'''\n",
    "Two 2-D convolutional blocks, each with two conv. layers with kernel size=3, max-pooling kernel=2. The first block has 64 feature maps while the second has 128\n",
    "'''\n",
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG motibated cnn module https://arxiv.org/pdf/1409.1556.pdf\n",
    "    code : https://github.com/pytorch/fairseq/blob/c36294ea4fd35eac757f417de9668b32c57d4b3d/fairseq/modules/vggblock.py#L38\n",
    "    Args:\n",
    "        in_channels: (int) number of input channels (typically 1)\n",
    "        out_channels: (int) number of output channels\n",
    "        conv_kernel_size: convolution channels\n",
    "        pooling_kernel_size: the size of the pooling window to take a max over\n",
    "        num_conv_layers: (int) number of convolution layers\n",
    "        input_dim: (int) input dimension\n",
    "        conv_stride: the stride of the convolving kernel.\n",
    "            Can be a single number or a tuple (sH, sW)  Default: 1\n",
    "        padding: implicit paddings on both sides of the input.\n",
    "            Can be a single number or a tuple (padH, padW). Default: None\n",
    "        layer_norm: (bool) if layer norm is going to be applied. Default: False\n",
    "    Shape:\n",
    "        Input: BxCxTxfeat, i.e. (batch_size, input_size, timesteps, features)\n",
    "        Output: BxCxTxfeat, i.e. (batch_size, input_size, timesteps, features)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                 conv_kernel_size,\n",
    "                 num_conv_layers,\n",
    "                 pooling_kernel_size,\n",
    "                 input_dim=None, \n",
    "                 conv_stride=1,\n",
    "                 padding=None,\n",
    "                 layer_norm=True\n",
    "                 ):\n",
    "        assert (\n",
    "            input_dim is not None\n",
    "        ), \"Need input_dim for LayerNorm and infer_conv_output_dim\"\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        \n",
    "        conv_kernel_size = _pair(conv_kernel_size)\n",
    "        pooling_kernel_size = _pair(pooling_kernel_size)\n",
    "        padding = (\n",
    "            tuple(e // 2 for e in conv_kernel_size)\n",
    "            if padding is None\n",
    "            else _pair(padding)\n",
    "        )\n",
    "        conv_stride = _pair(conv_stride)\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input_channels=1 : mfcc는 2d이므로 채널을 1로 봐야함.\n",
    "        # kernel size 3\n",
    "        for layer in range(num_conv_layers):\n",
    "            conv_op = nn.Conv2d(\n",
    "                in_channels if layer == 0 else out_channels,\n",
    "                out_channels,\n",
    "                conv_kernel_size,\n",
    "                stride=conv_stride,\n",
    "                padding=padding,\n",
    "            )\n",
    "            self.layers.append(conv_op)\n",
    "            if layer_norm:\n",
    "                self.conv_output_dim, per_channel_dim = infer_conv_output_dim(\n",
    "                    conv_op, input_dim, in_channels if layer == 0 else out_channels\n",
    "                )\n",
    "                self.layers.append(nn.LayerNorm(per_channel_dim))\n",
    "                input_dim = per_channel_dim\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        if pooling_kernel_size is not None:\n",
    "            # ceil_mode : when True, will use ceil instead of floor to compute the output shape\n",
    "            pool_op = nn.MaxPool2d(kernel_size=pooling_kernel_size, ceil_mode=True)\n",
    "            self.layers.append(pool_op)\n",
    "            self.total_output_dim, self.output_dim = infer_conv_output_dim(\n",
    "                pool_op, input_dim, out_channels\n",
    "            )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        for i, _ in enumerate(self.layers):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements simple/classical attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 1,\n",
    "        attn: str = 'cosine',\n",
    "        residual: bool = False,\n",
    "        get_weights: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if attn == 'cosine':\n",
    "            self.cosine = nn.CosineSimilarity(dim=dim)\n",
    "        self.attn = attn\n",
    "        self.dim = dim\n",
    "        self.get_weights = get_weights\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        xs: torch.Tensor,\n",
    "        ys: torch.Tensor,\n",
    "        mask_ys: Optional[torch.Tensor] = None,\n",
    "        values: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute attention.\n",
    "        Attend over ys with query xs to obtain weights, then apply weights to\n",
    "        values (ys if yalues is None)\n",
    "        Args:\n",
    "            xs: B x query_len x dim (queries)\n",
    "            ys: B x key_len x dim (keys)\n",
    "            mask_ys: B x key_len (mask)\n",
    "            values: B x value_len x dim (values); if None, default to ys\n",
    "        \"\"\"\n",
    "        bsz = xs.size(0)\n",
    "        y_len = ys.size(1)\n",
    "        x_len = xs.size(1)\n",
    "        if self.attn == 'cosine':\n",
    "            l1 = self.cosine(xs, ys).unsqueeze(self.dim - 1)\n",
    "        else:\n",
    "            l1 = torch.bmm(xs, ys.transpose(1, 2))\n",
    "            if self.attn == 'sqrt':\n",
    "                d_k = ys.size(-1)\n",
    "                l1 = l1 / math.sqrt(d_k)\n",
    "        if mask_ys is not None:\n",
    "            attn_mask = (mask_ys == 0).view(bsz, 1, y_len)\n",
    "            attn_mask = attn_mask.repeat(1, x_len, 1)\n",
    "            l1.masked_fill_(attn_mask, neginf(l1.dtype))\n",
    "        l2 = F.softmax(l1, dim=self.dim, dtype=torch.float).type_as(l1)\n",
    "        if values is None:\n",
    "            values = ys\n",
    "        lhs_emb = torch.bmm(l2, values)\n",
    "\n",
    "        # # add back the query\n",
    "        if self.residual:\n",
    "            lhs_emb = lhs_emb.add(xs)\n",
    "        \n",
    "        res = lhs_emb.squeeze(self.dim - 1)\n",
    "        if self.get_weights:\n",
    "            return res, l2\n",
    "        else:\n",
    "            return res\n",
    "        \n",
    "\n",
    "class PolyBasicAttention(BasicAttention):\n",
    "    \"\"\"\n",
    "    Override basic attention to account for edge case for polyencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, poly_type, n_codes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.poly_type = poly_type\n",
    "        self.n_codes = n_codes\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Account for accidental dimensionality reduction when num_codes is 1 and the\n",
    "        polyencoder type is 'codes'\n",
    "        \"\"\"\n",
    "        lhs_emb = super().forward(*args, **kwargs)\n",
    "        if self.poly_type == 'codes' and self.n_codes == 1 and len(lhs_emb.shape) == 2:\n",
    "            lhs_emb = lhs_emb.unsqueeze(self.dim - 1)\n",
    "        return lhs_emb\n",
    "    \n",
    "\n",
    "def neginf(dtype: torch.dtype) -> float:\n",
    "    \"\"\"\n",
    "    Return a representable finite number near -inf for a dtype.\n",
    "    \"\"\"\n",
    "    if dtype is torch.float16:\n",
    "        return -NEAR_INF_FP16\n",
    "    else:\n",
    "        return -NEAR_INF\n",
    "    \n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    '''\n",
    "    code : https://github.com/pytorch/fairseq/blob/c36294ea4fd35eac757f417de9668b32c57d4b3d/examples/speech_recognition/models/vggtransformer.py#L271\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 poly_n_codes, # m, the number of global contxt features\n",
    "                 poly_attention_num_heads,\n",
    "                 num_att_layers,\n",
    "                 codes_attention_num_heads,\n",
    "                 embed_dim, \n",
    "                \n",
    "                 input_feat_per_channel,\n",
    "                 num_conv_block,\n",
    "                 num_conv_layers, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                 conv_kernel_size=3, \n",
    "                 pooling_kernel_size=2,\n",
    "                 layer_norm=False,\n",
    "                 dropout=0.1,\n",
    "                 reduction_type='first' # first, avg, max\n",
    "                ):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "                \n",
    "        self.in_channels = in_channels\n",
    "        self.input_dim = input_feat_per_channel\n",
    "        self.reduction_type = reduction_type\n",
    "        \n",
    "        self.conv_encoder_block = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_conv_block):\n",
    "            self.conv_encoder_block.append(\n",
    "                ConvEncoder(\n",
    "                    in_channels=in_channels, \n",
    "                    out_channels=out_channels,\n",
    "                    conv_kernel_size=conv_kernel_size,\n",
    "                    num_conv_layers=num_conv_layers,\n",
    "                    pooling_kernel_size=pooling_kernel_size,\n",
    "                    input_dim=input_feat_per_channel,\n",
    "                    layer_norm=layer_norm\n",
    "                    )\n",
    "                )\n",
    "            in_channels = out_channels\n",
    "            input_feat_per_channel = self.conv_encoder_block[-1].output_dim\n",
    "        \n",
    "        self.conv_encoder_block = nn.Sequential(*self.conv_encoder_block)\n",
    "        \n",
    "        # conv_output_dim is the output dimension of conv encoder\n",
    "        conv_output_dim = self.infer_conv_output_dim(self.in_channels, self.input_dim)\n",
    "        \n",
    "        self.n_codes = poly_n_codes\n",
    "        self.attention_num_heads = poly_attention_num_heads\n",
    "        self.codes_attention_num_heads = codes_attention_num_heads\n",
    "\n",
    "        # the codes\n",
    "        codes = torch.empty(self.n_codes, embed_dim)\n",
    "        codes = torch.nn.init.uniform_(codes)\n",
    "        self.codes = torch.nn.Parameter(codes)\n",
    "\n",
    "        # attention for the codes\n",
    "        self.code_attention = PolyBasicAttention(poly_type='codes', n_codes=self.n_codes, dim=2, attn='basic', get_weights=False)\n",
    "\n",
    "        # The final attention (the one that takes the candidate as key)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, \n",
    "                                               num_heads=self.attention_num_heads, \n",
    "                                               dropout=0.0)\n",
    "\n",
    "        \n",
    "        self.att_encoder_block = nn.ModuleList()\n",
    "        \n",
    "        # conv encoder를 거쳐나온 데이터의 dim과 embed_dim이 다를 경우 맞춰준다\n",
    "        if conv_output_dim != embed_dim:\n",
    "            self.att_encoder_block.append(nn.Linear(conv_output_dim, embed_dim))\n",
    "        \n",
    "        # SelfAttentionEncoder * num_att_layers\n",
    "        for i in range(num_att_layers):\n",
    "            self.att_encoder_block.append(\n",
    "                torch.nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                 nhead=self.attention_num_heads, \n",
    "                                                 dim_feedforward=embed_dim*4, \n",
    "                                                 dropout=dropout, \n",
    "                                                 activation='gelu')\n",
    "            )\n",
    "                            \n",
    "        self.att_encoder_block = nn.Sequential(*self.att_encoder_block)\n",
    "\n",
    "            \n",
    "    def attend(self, attention_layer, queries, keys, values, mask):\n",
    "        \"\"\"\n",
    "        Apply attention.\n",
    "        :param attention_layer:\n",
    "            nn.Module attention layer to use for the attention\n",
    "        :param queries:\n",
    "            the queries for attention\n",
    "        :param keys:\n",
    "            the keys for attention\n",
    "        :param values:\n",
    "            the values for attention\n",
    "        :param mask:\n",
    "            mask for the attention keys\n",
    "        :return:\n",
    "            the result of applying attention to the values, with weights computed\n",
    "            wrt to the queries and keys.\n",
    "        \"\"\"\n",
    "        if keys is None:\n",
    "            keys = values\n",
    "        if isinstance(attention_layer, PolyBasicAttention):\n",
    "            return attention_layer(queries, keys, values=values, mask_ys=mask)\n",
    "        elif isinstance(attention_layer, nn.MultiheadAttention):\n",
    "            return attention_layer(query=queries, key=keys, value=values, attn_mask=mask)[0]\n",
    "\n",
    "        else:\n",
    "            raise Exception('Unrecognized type of attention')\n",
    "\n",
    "    def encode(self, x_raw):\n",
    "            # x_raw = [current song, next song]\n",
    "            # next: candidate\n",
    "            cs, ns, label = x_raw\n",
    "\n",
    "            # padded tensor\n",
    "            # B, C, T, F\n",
    "            bsz, in_channels, max_seq_len, _ = ns.size()\n",
    "        \n",
    "            # cand mfcc를 conv encoder를 거친 emb\n",
    "            cand_emb = self.conv_encoder_block(ns)\n",
    "                        \n",
    "            bsz, _, output_seq_len, _ = cand_emb.size()\n",
    "\n",
    "            # (B, C, T, F) -> (B, T, C, F) -> (B, T, C*F)\n",
    "            cand_emb = cand_emb.transpose(1, 2)\n",
    "            cand_emb = cand_emb.contiguous().view(bsz, output_seq_len, -1)\n",
    "\n",
    "            # transformer encoder\n",
    "            cand_emb = self.att_encoder_block(cand_emb)\n",
    "            \n",
    "            # reduction : first, avg, max\n",
    "            if self.reduction_type=='first':\n",
    "                cand_emb = cand_emb[:,0,:]\n",
    "            elif self.reduction_type == 'avg':\n",
    "                cand_emb = torch.mean(cand_emb, dim=1)\n",
    "            elif self.reduction_type == 'max':\n",
    "                cand_emb = torch.max(cand_emb, dim=1).values\n",
    "            else:\n",
    "                raise KeyError('Not Registered reduction_type. Capable options : first, avg, and max')\n",
    "            cand_emb = cand_emb.view(cand_emb.size()[0], 1, cand_emb.size()[1])\n",
    "            \n",
    "            # ctxt mfcc를 conv encoder를 거친 emb\n",
    "            ctxt_out = self.conv_encoder_block(cs)\n",
    "            b, c, t, f = ctxt_out.size()\n",
    "            \n",
    "            # (B, C, T, F) -> (B, T, C, F) -> (B, T, C*F)\n",
    "            ctxt_out = ctxt_out.transpose(1, 2)\n",
    "            ctxt_out = ctxt_out.contiguous().view(b, t, -1)\n",
    "            \n",
    "            # transformer encoder\n",
    "            ctxt_out = self.att_encoder_block(ctxt_out)\n",
    "            \n",
    "            return ctxt_out, cand_emb, label\n",
    "                \n",
    "    def forward(self, \n",
    "                x_raw=None, \n",
    "                x_rep=None\n",
    "               ):\n",
    "        '''\n",
    "        encoding 과정과 그 이후 과정을 나눈 이유는 먼저 계속 사용하는 피쳐를 처리해놓고 재사용하는 과정을 거치기 위해서이다.\n",
    "        '''\n",
    "        if x_raw is not None:\n",
    "            \n",
    "            return self.encode(x_raw)\n",
    "        elif x_rep is not None:\n",
    "            ctxt_out, cand_emb = x_rep\n",
    "\n",
    "            # m개 만큼 context code를 반복\n",
    "            # ctxt_out 값과 code를 내적한 값들의 softmax한 벡터 (w_1,...,w_m)를 이전 레이어 결과값(ctxt_out)과 곱해서 합한다.\n",
    "            # 이 값이 m개의 global context features\n",
    "            bsz = cand_emb.size(0)\n",
    "            global_ctxts = self.attend(attention_layer=self.code_attention , \n",
    "                                       queries=self.codes.repeat(bsz, 1, 1), \n",
    "                                       keys=ctxt_out,\n",
    "                                       values=ctxt_out, \n",
    "                                       mask=None) \n",
    "            \n",
    "            # torch.nn.MultiheadAttention은 (seq_len, bsz, embed_dim) 으로 입력값을 받아서 수정\n",
    "#             global_ctxts = global_ctxts.transpose(0,1)\n",
    "            #cand_emb = cand_emb.transpose(0,1)\n",
    "\n",
    "            # m개의 global context features를 cand_emb와 내적한 값을 softmax한 벡터를 (w_1,...,w_m)라 할 때, 이 가중치 값과 global contxt features를 곱해서 합한다.\n",
    "            # 이 값이 최종 ctxt_emb\n",
    "            ctxt_emb = self.attend(attention_layer=self.attention ,\n",
    "                                   queries=cand_emb,\n",
    "                                   keys=global_ctxts,\n",
    "                                   values=global_ctxts,\n",
    "                                   mask=None)        \n",
    "\n",
    "            # score: cand_emb와 ctxt_emb 간 cosine similarity값 (반환값)\n",
    "            scores = torch.sum(ctxt_emb * cand_emb, 2)\n",
    "            return scores\n",
    "        else:\n",
    "            raise Exception('Unsupported operation')\n",
    "    \n",
    "    def infer_conv_output_dim(self, in_channels, input_dim):\n",
    "        sample_seq_len = 200\n",
    "        sample_bsz = 10\n",
    "        x = torch.randn(sample_bsz, in_channels, sample_seq_len, input_dim)\n",
    "        for i, _ in enumerate(self.conv_encoder_block):\n",
    "            x = self.conv_encoder_block[i](x)\n",
    "        # (B, C, T, F) -> (B, T, C, F) -> (B, T, C*F)\n",
    "        x = x.transpose(1, 2)\n",
    "        mb, seq = x.size()[:2]\n",
    "        return x.contiguous().view(mb, seq, -1).size(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74b2c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioEncoder(\n",
    "                     poly_n_codes=64, # m, the number of global contxt features\n",
    "                     poly_attention_num_heads=8, \n",
    "                     codes_attention_num_heads=2,\n",
    "                     num_att_layers=6,\n",
    "                     embed_dim=512, \n",
    "                     input_feat_per_channel=40, # feature vector dimension\n",
    "                     num_conv_block=2,\n",
    "                     num_conv_layers=2, \n",
    "                     in_channels=1, \n",
    "                     out_channels=32,\n",
    "                     conv_kernel_size=3, \n",
    "                     pooling_kernel_size=2,\n",
    "                     layer_norm=True,\n",
    "                     reduction_type='first'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd06af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072292ab",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fd1fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_check = None\n",
    "\n",
    "def eval_model(dataloader, criterion):\n",
    "    # layer 중 dropout layer와 같이 학습 시에는 사용하는데 inference할 때는 사용하지 않는 경우를 구분해주기 위함\n",
    "    model.eval()\n",
    "    \n",
    "    n_steps = 0\n",
    "    eval_loss = 0\n",
    "    \n",
    "#     batch_labels = []\n",
    "#     scores = []\n",
    "    \n",
    "    acc = 0\n",
    "#     labels = []\n",
    "\n",
    "    # autograd engine을 끔(gradient 계산하지 않음) --> 메모리 사용량을 줄이고 연산 속도를 높이기 위함\n",
    "    with torch.no_grad():\n",
    "        # input은 이미 1차 임베딩은 되어 있는 것을 가져왔다고 가정한다.\n",
    "        for idx, (cs_rep, ns_rep, label) in enumerate(dataloader):\n",
    "            n_steps+=1\n",
    "#             batch_labels.append(torch.LongTensor([torch.argmax(batch_label).item()]))\n",
    "#             batch_labels.append(torch.argmax(label).item())\n",
    "#             scores.append(model(x_rep=[cs_rep, ns_rep]))\n",
    "            batch_label = torch.LongTensor([torch.argmax(label).item()]).to(device_num)\n",
    "            score = model(x_rep=[cs_rep, ns_rep])\n",
    "            print(f'eval score : {score}')\n",
    "#             if eval_check is not None :\n",
    "#                 if eval_check == score:\n",
    "#                     print('same score!!!! \\n')\n",
    "#             eval_check = score\n",
    "                \n",
    "            loss = criterion(score, batch_label)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "#             print(f'score : {torch.argmax(score).item()}')\n",
    "#             print(f'label : {batch_label.item()}')\n",
    "            if torch.argmax(score).item() == batch_label.item():\n",
    "                acc += 1\n",
    "\n",
    "    results = {\n",
    "        'eval_accuracy': acc / n_steps,\n",
    "        'eval_loss': eval_loss / n_steps\n",
    "    }\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11faa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def train(model, raw_data, epochs, batch_size, output_dir, lr, betas=(0.9, 0.999), weight_decay: float = 0.01, val_ratio=0.2, fp16=False, fp16_opt_level='O1'):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    \n",
    "    # fp16=True면 amp를 통한 mixed preicision training을 한다는 의미\n",
    "    # 사용 조건 : Volta 이상의 nvidia 그래픽 카드(v100, rtx2080ti, 등)\n",
    "    if fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=fp16_opt_level)\n",
    "    \n",
    "    def train_batch(data, bsz, shuffle=True):\n",
    "        cs_rep, ns_rep, labels = data\n",
    "        length = cs_rep.size()[0]\n",
    "        \n",
    "        for i in range(0, length, bsz):\n",
    "            cs_batch = cs_rep[i:i+bsz]\n",
    "            ns_batch = ns_rep[i:i+bsz]\n",
    "            label_batch = labels[i:i+bsz]\n",
    "            \n",
    "            if shuffle:\n",
    "                indexes = torch.randperm(bsz)              \n",
    "                yield [cs_batch[indexes], ns_batch[indexes], label_batch[indexes]]\n",
    "            else:\n",
    "                yield [cs_batch, ns_batch, label_batch]    \n",
    "    state_save_path = os.path.join(output_dir, '{}_{}_{}_pytorch_model.bin'.format('polyencoder', '64', '08081940'))\n",
    "        \n",
    "    best_eval_loss = float('inf')\n",
    "    best_test_loss = float('inf')\n",
    "    \n",
    "    cs, ns, labels = raw_data\n",
    "        \n",
    "    init_time = time.time()\n",
    "    # 재사용 가능한 임베딩들은 미리 뽑기\n",
    "    trainset = None\n",
    "    valset = None\n",
    "    if val_ratio > 0:\n",
    "        train_len = int((1-val_ratio)*len(labels))\n",
    "        trainset = [cs[:train_len], ns[:train_len], labels[:train_len]]\n",
    "        valset = [cs[train_len:], ns[train_len:], labels[train_len:]]\n",
    "    else:\n",
    "        trainset = raw_data\n",
    "\n",
    "    eval_freq = len(trainset[2]) // 3\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#             optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "\n",
    "    # Set the learning rate of each parameter group using a cosine annealing schedule,\n",
    "    # https://pytorch.org/docs/1.8.0/optim.html?highlight=cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    # stepRL\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=eval_freq, gamma=0.95)\n",
    "\n",
    "        \n",
    "    batch_labels = []\n",
    "    scores = []\n",
    "    global_step = 0\n",
    "    #TODO : 나중에 tqdm으로 바꾸기\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        train_batches = train_batch(trainset, batch_size)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        local_step = 0\n",
    "        for idx, (cs_rep, ns_rep, batch_label) in enumerate(train_batches):\n",
    "            model.train()\n",
    "            # clear gradient\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # mfcc embedding encoder\n",
    "#             cs_rep, ns_rep, label = model(x_raw=[batch_cs, batch_ns, batch_label])\n",
    "            \n",
    "            # label\n",
    "            batch_label = torch.LongTensor([torch.argmax(batch_label).item()]).to(device_num)\n",
    "            \n",
    "            # poly encoder\n",
    "            scores = model(x_rep=[cs_rep, ns_rep])\n",
    "#             scores = F.softmax(scores)\n",
    "#             print(f'scores : {scores}')\n",
    "\n",
    "            # cross entropy\n",
    "            loss = criterion(scores, batch_label)\n",
    "                \n",
    "#             loss = (batch_label * -torch.log(scores)).sum(dim=0).mean()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predict = torch.argmax(scores)\n",
    "#             print('predict : ',predict.item())\n",
    "#             print(f'batch_label.item() : {batch_label.item()}')\n",
    "#             if predict == torch.argmax(batch_label).item():\n",
    "            if predict.item() == batch_label.item():\n",
    "                accuracy += 1\n",
    "#                 print(f'-- correct : {accuracy}')\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(f'scores : {scores}')\n",
    "            \n",
    "            if fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            if global_step and global_step % eval_freq == 0:\n",
    "                if valset is not None:\n",
    "                    val_batches = train_batch(valset, batch_size, shuffle=False)\n",
    "\n",
    "                eval_results = eval_model(val_batches, criterion)\n",
    "\n",
    "                print(f'Global Step {global_step} Eval loss : {eval_results[\"eval_loss\"]}, Eval accuracy : {eval_results[\"eval_accuracy\"]}')\n",
    "                if eval_results[\"eval_loss\"] < best_eval_loss:\n",
    "                    best_eval_loss = eval_results['eval_loss']\n",
    "                    # save model.state_dict()\n",
    "                    torch.save(model.state_dict(), state_save_path)\n",
    "#                     print(f'[Saving at] {state_save_path}')\n",
    "\n",
    "            \n",
    "            global_step += 1\n",
    "            local_step += 1\n",
    "\n",
    "            #print(f'batch {idx} | Loss {loss.item()}')\n",
    "        \n",
    "        total_loss = total_loss / local_step\n",
    "        train_accuracy = accuracy / local_step\n",
    "        print(f'\\n[Epoch {epoch}] | train_cost {total_loss} | train_accuracy {train_accuracy} | time {time.time()-start_time}\\n')\n",
    "        \n",
    "        if valset is not None:\n",
    "            val_batches = train_batch(valset, batch_size, shuffle=False)\n",
    "        eval_results = eval_model(val_batches, criterion)\n",
    "        print(f'Eval loss : {eval_results[\"eval_loss\"]}, Eval accuracy : {eval_results[\"eval_accuracy\"]}')\n",
    "        if eval_results[\"eval_loss\"] < best_eval_loss:\n",
    "            best_eval_loss = eval_results['eval_loss']\n",
    "            # save model.state_dict()\n",
    "            torch.save(model.state_dict(), state_save_path)\n",
    "#             print(f'[Saving at] {state_save_path}')\n",
    "        print('---------------------------------\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58ed9f03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batch2 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4cc9129b8f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#             optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m train(model=model.to(device_num), \n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mraw_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-e81887bffb7a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, raw_data, epochs, batch_size, output_dir, lr, betas, weight_decay, val_ratio, fp16, fp16_opt_level)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# poly encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcs_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_rep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;31m#             scores = F.softmax(scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#             print(f'scores : {scores}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-af578c46b388>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_raw, x_rep)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# 이 값이 m개의 global context features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mbsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcand_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             global_ctxts = self.attend(attention_layer=self.code_attention , \n\u001b[0m\u001b[1;32m    397\u001b[0m                                        \u001b[0mqueries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctxt_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-af578c46b388>\u001b[0m in \u001b[0;36mattend\u001b[0;34m(self, attention_layer, queries, keys, values, mask)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolyBasicAttention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_ys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/envs/venv_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-af578c46b388>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mpolyencoder\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m'codes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mlhs_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'codes'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_codes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mlhs_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlhs_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-af578c46b388>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs, ys, mask_ys, values)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0md_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch2 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "# RuntimeError: Expected 4-dimensional input for 4-dimensional weight [256, 1, 3, 3], but got 3-dimensional input of size [320, 40, 2800] instead\n",
    "# tdata = [now_mfcc_list.to(1), next_mfcc_list.to(1), label_list.to(1)]\n",
    "tdata = [cs.to(device_num), ns.to(device_num), labels.to(device_num)]\n",
    "# batch_size, C, H, W = 32, 1, 28, 28\n",
    "# tdata = [torch.randn(batch_size, C, H, W).to(1),torch.randn(batch_size, C, H, W).to(1), torch.randn(batch_size).to(1)]\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 20 # epoch\n",
    "batch_size=32 # batch size for training\n",
    "lr=0.01 # learning rate\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#             optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "\n",
    "train(model=model.to(device_num), \n",
    "      raw_data=tdata, \n",
    "      epochs=epochs, \n",
    "      batch_size=batch_size, \n",
    "      output_dir='.', \n",
    "      lr=lr,\n",
    "      val_ratio=0.2, \n",
    "      fp16=False, \n",
    "      fp16_opt_level='O1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967a8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27768ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-7abfc511be6b>:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.Tensor([[0.8982, 0.805, 0.6393, 0.9983, 0.5731, 0.0469, 0.556, 0.1476, 0.8404, 0.5544]])\n",
    "x = F.softmax(x)\n",
    "print(x.size())\n",
    "y = torch.LongTensor([1])\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af893fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2857)\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "print(cross_entropy_loss(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5e71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb308e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-023fa0d9b115>:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[2, 0.805, 0.6393]])\n",
    "x = F.softmax(x)\n",
    "print(x.size())\n",
    "y = torch.LongTensor([0])\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5cdaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8154)\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "print(cross_entropy_loss(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91d6a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2626)\n"
     ]
    }
   ],
   "source": [
    "y1 = torch.LongTensor([1])\n",
    "print(cross_entropy_loss(x, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a63290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2923)\n"
     ]
    }
   ],
   "source": [
    "y2 = torch.LongTensor([2])\n",
    "print(cross_entropy_loss(x, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8acf850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0357,  1.4852,  0.3566,  0.7393])\n",
      "tensor([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4)\n",
    "print(a)\n",
    "a = torch.LongTensor([torch.argmax(a).item()])\n",
    "print(a)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "815e9eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.LongTensor([1])\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7d26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[2.0959e-02, 2.0501e-04, 1.5565e-02, 5.2595e-02, 1.3077e-05, 1.2790e-03,\n",
    "         3.7969e-01, 3.6344e-05, 1.1723e-01, 5.2833e-02, 2.7906e-05, 2.4717e-04,\n",
    "         4.2139e-05, 3.3961e-04, 1.4989e-05, 1.3840e-03, 1.6910e-02, 1.2072e-03,\n",
    "         2.4667e-02, 2.1032e-03, 2.3573e-01, 3.0013e-02, 6.1480e-04, 1.3032e-02,\n",
    "         7.4176e-03, 1.4105e-02, 5.0791e-04, 9.9195e-04, 2.5835e-05, 7.1133e-03,\n",
    "         6.1536e-04, 2.4788e-03]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca627b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b97542",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.LongTensor([6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9c98c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48d7d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4793)\n",
      "tensor(3.5000)\n",
      "tensor(3.4847)\n",
      "tensor(3.4477)\n",
      "tensor(3.5002)\n",
      "tensor(3.4990)\n",
      "tensor(3.1206)\n",
      "tensor(3.5002)\n",
      "tensor(3.3830)\n",
      "tensor(3.4474)\n",
      "tensor(3.5002)\n",
      "tensor(3.5000)\n",
      "tensor(3.5002)\n",
      "tensor(3.4999)\n",
      "tensor(3.5002)\n",
      "tensor(3.4989)\n",
      "tensor(3.4833)\n",
      "tensor(3.4990)\n",
      "tensor(3.4756)\n",
      "tensor(3.4981)\n",
      "tensor(3.2645)\n",
      "tensor(3.4702)\n",
      "tensor(3.4996)\n",
      "tensor(3.4872)\n",
      "tensor(3.4928)\n",
      "tensor(3.4861)\n",
      "tensor(3.4997)\n",
      "tensor(3.4993)\n",
      "tensor(3.5002)\n",
      "tensor(3.4931)\n",
      "tensor(3.4996)\n",
      "tensor(3.4978)\n"
     ]
    }
   ],
   "source": [
    "for i in range(x.size()[1]):\n",
    "    y = torch.LongTensor([i])\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    print(cross_entropy_loss(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae21cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
